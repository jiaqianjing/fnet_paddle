{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a2b65876-a093-472b-b6e4-bf0dc578be1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys\n",
    "import paddle\n",
    "sys.path.append('/workspace/fnet_paddle/PaddleNLP')\n",
    "from paddlenlp.datasets import load_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a054e2f7-b649-4323-a237-da478d0982d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:paddle.utils.download:unique_endpoints {''}\n"
     ]
    }
   ],
   "source": [
    "train_ds, dev_ds = load_dataset(\"glue\", name=\"cola\", splits=(\"train\", \"dev\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "33540c1f-1d78-49e0-b971-4e20b2778d74",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([{'sentence': \"Our friends won't buy this analysis, let alone the next one we propose.\",\n",
       "   'labels': 1},\n",
       "  {'sentence': \"One more pseudo generalization and I'm giving up.\",\n",
       "   'labels': 1},\n",
       "  {'sentence': \"One more pseudo generalization or I'm giving up.\",\n",
       "   'labels': 1}],\n",
       " [{'sentence': 'The sailors rode the breeze clear of the rocks.', 'labels': 1},\n",
       "  {'sentence': 'The weights made the rope stretch over the pulley.',\n",
       "   'labels': 1},\n",
       "  {'sentence': 'The mechanical doll wriggled itself loose.', 'labels': 1}])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_ds[:3], dev_ds[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e2f73639-d62f-4f05-9a13-096103980c5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_example(example,\n",
    "                    tokenizer,\n",
    "                    max_seq_length=512,\n",
    "                    is_test=False):\n",
    "    text = example[\"sentence\"]\n",
    "    text_pair = None\n",
    "    encoded_inputs = tokenizer(\n",
    "        text=text, text_pair=text_pair, max_seq_len=max_seq_length)\n",
    "    input_ids = encoded_inputs[\"input_ids\"]\n",
    "    token_type_ids = encoded_inputs[\"token_type_ids\"]\n",
    "\n",
    "    if is_test:\n",
    "        return input_ids, token_type_ids\n",
    "    label = np.array([example[\"labels\"]], dtype=\"int64\")\n",
    "    return input_ids, token_type_ids, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "14e4df59-fb33-4122-8358-f7e15137cd18",
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "import argparse\n",
    "import os\n",
    "import random\n",
    "import time\n",
    "import distutils.util\n",
    "import numpy as np\n",
    "import paddle\n",
    "import paddle.nn.functional as F\n",
    "import paddlenlp as ppnlp\n",
    "from paddlenlp.data import Stack, Tuple, Pad\n",
    "from paddlenlp.datasets import load_dataset\n",
    "from paddlenlp.transformers import LinearDecayWithWarmup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1f639139-6ce0-4b72-ad93-8aca3b3ff84a",
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument(\"--save_dir\", default='./checkpoint', type=str, help=\"The output directory where the model checkpoints will be written.\")\n",
    "parser.add_argument(\"--max_seq_length\", default=128, type=int, help=\"The maximum total input sequence length after tokenization. \"\n",
    "    \"Sequences longer than this will be truncated, sequences shorter will be padded.\")\n",
    "parser.add_argument(\"--batch_size\", default=32, type=int, help=\"Batch size per GPU/CPU for training.\")\n",
    "parser.add_argument(\"--learning_rate\", default=5e-5, type=float, help=\"The initial learning rate for Adam.\")\n",
    "parser.add_argument(\"--weight_decay\", default=0.0, type=float, help=\"Weight decay if we apply some.\")\n",
    "parser.add_argument(\"--epochs\", default=3, type=int, help=\"Total number of training epochs to perform.\")\n",
    "parser.add_argument(\"--warmup_proportion\", default=0.0, type=float, help=\"Linear warmup proption over the training process.\")\n",
    "parser.add_argument(\"--valid_steps\", default=100, type=int, help=\"The interval steps to evaluate model performance.\")\n",
    "parser.add_argument(\"--save_steps\", default=100, type=int, help=\"The interval steps to save checkppoints.\")\n",
    "parser.add_argument(\"--logging_steps\", default=10, type=int, help=\"The interval steps to logging.\")\n",
    "parser.add_argument(\"--init_from_ckpt\", type=str, default=None, help=\"The path of checkpoint to be loaded.\")\n",
    "parser.add_argument(\"--seed\", type=int, default=1000, help=\"random seed for initialization\")\n",
    "parser.add_argument('--device', choices=['cpu', 'gpu', 'xpu', 'npu'], default=\"gpu\", help=\"Select which device to train model, defaults to gpu.\")\n",
    "parser.add_argument(\"--use_amp\", type=distutils.util.strtobool, default=False, help=\"Enable mixed precision training.\")\n",
    "parser.add_argument(\"--scale_loss\", type=float, default=2**15, help=\"The value of scale_loss for fp16.\")\n",
    "args = parser.parse_args([])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cb89ede1-4449-4e9c-8ce4-7802c9d1507f",
   "metadata": {},
   "outputs": [],
   "source": [
    "args.seed = 999\n",
    "args.batch_size = 64\n",
    "args.epochs = 3\n",
    "args.learning_rate=2.5e-03"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b439e14d-68ca-46d9-b529-cc3caa2d27f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_seed(seed):\n",
    "    \"\"\"sets random seed\"\"\"\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    paddle.seed(seed)\n",
    "\n",
    "\n",
    "@paddle.no_grad()\n",
    "def evaluate(model, criterion, metric, data_loader):\n",
    "    \"\"\"\n",
    "    Given a dataset, it evals model and computes the metric.\n",
    "    Args:\n",
    "        model(obj:`paddle.nn.Layer`): A model to classify texts.\n",
    "        data_loader(obj:`paddle.io.DataLoader`): The dataset loader which generates batches.\n",
    "        criterion(obj:`paddle.nn.Layer`): It can compute the loss.\n",
    "        metric(obj:`paddle.metric.Metric`): The evaluation metric.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    metric.reset()\n",
    "    losses = []\n",
    "    for batch in data_loader:\n",
    "        input_ids, token_type_ids, labels = batch\n",
    "        logits = model(input_ids, token_type_ids)\n",
    "        loss = criterion(logits, labels)\n",
    "        losses.append(loss.numpy())\n",
    "        correct = metric.compute(logits, labels)\n",
    "        metric.update(correct)\n",
    "    accu = metric.accumulate()\n",
    "    print(\"eval loss: %.5f, accu: %.5f\" % (np.mean(losses), accu))\n",
    "    model.train()\n",
    "    metric.reset()\n",
    "\n",
    "\n",
    "def create_dataloader(dataset,\n",
    "                      mode='train',\n",
    "                      batch_size=1,\n",
    "                      batchify_fn=None,\n",
    "                      trans_fn=None):\n",
    "    if trans_fn:\n",
    "        dataset = dataset.map(trans_fn)\n",
    "\n",
    "    shuffle = True if mode == 'train' else False\n",
    "    if mode == 'train':\n",
    "        batch_sampler = paddle.io.DistributedBatchSampler(\n",
    "            dataset, batch_size=batch_size, shuffle=shuffle)\n",
    "    else:\n",
    "        batch_sampler = paddle.io.BatchSampler(\n",
    "            dataset, batch_size=batch_size, shuffle=shuffle)\n",
    "\n",
    "    return paddle.io.DataLoader(\n",
    "        dataset=dataset,\n",
    "        batch_sampler=batch_sampler,\n",
    "        collate_fn=batchify_fn,\n",
    "        return_list=True)\n",
    "\n",
    "\n",
    "def do_train():\n",
    "    paddle.set_device(args.device)\n",
    "    rank = paddle.distributed.get_rank()\n",
    "    if paddle.distributed.get_world_size() > 1:\n",
    "        paddle.distributed.init_parallel_env()\n",
    "\n",
    "    set_seed(args.seed)\n",
    "\n",
    "    train_ds, dev_ds = load_dataset(\"glue\", name=\"cola\", splits=(\"train\", \"dev\"))\n",
    "\n",
    "    fnet = ppnlp.transformers.FNetModel.from_pretrained('pretrained_model/paddle/large')\n",
    "    model = ppnlp.transformers.FNetForSequenceClassification(fnet, num_classes=len(train_ds.label_list))\n",
    "    tokenizer = ppnlp.transformers.FNetTokenizer.from_pretrained('fnet-large')\n",
    "\n",
    "    trans_func = partial(\n",
    "        convert_example,\n",
    "        tokenizer=tokenizer,\n",
    "        max_seq_length=args.max_seq_length)\n",
    "    batchify_fn = lambda samples, fn=Tuple(\n",
    "        Pad(axis=0, pad_val=tokenizer.pad_token_id),  # input\n",
    "        Pad(axis=0, pad_val=tokenizer.pad_token_type_id),  # segment\n",
    "        Stack(dtype=\"int64\")  # label\n",
    "    ): [data for data in fn(samples)]\n",
    "    train_data_loader = create_dataloader(\n",
    "        train_ds,\n",
    "        mode='train',\n",
    "        batch_size=args.batch_size,\n",
    "        batchify_fn=batchify_fn,\n",
    "        trans_fn=trans_func)\n",
    "    dev_data_loader = create_dataloader(\n",
    "        dev_ds,\n",
    "        mode='dev',\n",
    "        batch_size=args.batch_size,\n",
    "        batchify_fn=batchify_fn,\n",
    "        trans_fn=trans_func)\n",
    "\n",
    "    if args.init_from_ckpt and os.path.isfile(args.init_from_ckpt):\n",
    "        state_dict = paddle.load(args.init_from_ckpt)\n",
    "        model.set_dict(state_dict)\n",
    "    model = paddle.DataParallel(model)\n",
    "\n",
    "    num_training_steps = len(train_data_loader) * args.epochs\n",
    "\n",
    "    lr_scheduler = LinearDecayWithWarmup(args.learning_rate, num_training_steps,\n",
    "                                         args.warmup_proportion)\n",
    "\n",
    "    # Generate parameter names needed to perform weight decay.\n",
    "    # All bias and LayerNorm parameters are excluded.\n",
    "    decay_params = [\n",
    "        p.name for n, p in model.named_parameters()\n",
    "        if not any(nd in n for nd in [\"bias\", \"norm\"])\n",
    "    ]\n",
    "    optimizer = paddle.optimizer.AdamW(\n",
    "        learning_rate=lr_scheduler,\n",
    "        parameters=model.parameters(),\n",
    "        weight_decay=args.weight_decay,\n",
    "        apply_decay_param_fun=lambda x: x in decay_params)\n",
    "\n",
    "    criterion = paddle.nn.loss.CrossEntropyLoss()\n",
    "    metric = paddle.metric.Accuracy()\n",
    "    if args.use_amp:\n",
    "        scaler = paddle.amp.GradScaler(init_loss_scaling=args.scale_loss)\n",
    "    global_step = 0\n",
    "    tic_train = time.time()\n",
    "    for epoch in range(1, args.epochs + 1):\n",
    "        for step, batch in enumerate(train_data_loader, start=1):\n",
    "            input_ids, token_type_ids, labels = batch\n",
    "            with paddle.amp.auto_cast(\n",
    "                    args.use_amp,\n",
    "                    custom_white_list=[\"layer_norm\", \"softmax\", \"gelu\"], ):\n",
    "                logits = model(input_ids, token_type_ids)\n",
    "                loss = criterion(logits, labels)\n",
    "            probs = F.softmax(logits, axis=1)\n",
    "            correct = metric.compute(probs, labels)\n",
    "            metric.update(correct)\n",
    "            acc = metric.accumulate()\n",
    "\n",
    "            global_step += 1\n",
    "            if global_step % args.logging_steps == 0 and rank == 0:\n",
    "                print(\n",
    "                    \"global step %d, epoch: %d, batch: %d, loss: %.5f, accu: %.5f, speed: %.2f step/s\"\n",
    "                    % (global_step, epoch, step, loss, acc,\n",
    "                       10 / (time.time() - tic_train)))\n",
    "                tic_train = time.time()\n",
    "            if args.use_amp:\n",
    "                scaler.scale(loss).backward()\n",
    "                scaler.minimize(optimizer, loss)\n",
    "            else:\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "            lr_scheduler.step()\n",
    "            optimizer.clear_grad()\n",
    "\n",
    "            if global_step % args.valid_steps == 0 and rank == 0:\n",
    "                evaluate(model, criterion, metric, dev_data_loader)\n",
    "\n",
    "            if global_step % args.save_steps == 0 and rank == 0:\n",
    "                save_dir = os.path.join(args.save_dir, \"model_%d\" % global_step)\n",
    "                if not os.path.exists(save_dir):\n",
    "                    os.makedirs(save_dir)\n",
    "                model._layers.save_pretrained(save_dir)\n",
    "                tokenizer.save_pretrained(save_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "12c441f5-660e-4f93-83d2-bd3cc20e41c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:paddle.utils.download:unique_endpoints {''}\n",
      "\u001b[32m[2021-11-28 00:38:55,918] [    INFO]\u001b[0m - Already cached /root/.paddlenlp/models/fnet-large/spiece.model\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "global step 10, epoch: 1, batch: 10, loss: 1.14025, accu: 0.50000, speed: 2.40 step/s\n",
      "global step 20, epoch: 1, batch: 20, loss: 0.63458, accu: 0.59297, speed: 2.15 step/s\n",
      "global step 30, epoch: 1, batch: 30, loss: 0.70803, accu: 0.62448, speed: 2.42 step/s\n",
      "global step 40, epoch: 1, batch: 40, loss: 0.79939, accu: 0.61680, speed: 2.27 step/s\n",
      "global step 50, epoch: 1, batch: 50, loss: 0.61969, accu: 0.63094, speed: 2.52 step/s\n",
      "global step 60, epoch: 1, batch: 60, loss: 0.83507, accu: 0.63177, speed: 2.14 step/s\n",
      "global step 70, epoch: 1, batch: 70, loss: 0.52808, accu: 0.63862, speed: 2.27 step/s\n",
      "global step 80, epoch: 1, batch: 80, loss: 0.62606, accu: 0.64668, speed: 2.33 step/s\n",
      "global step 90, epoch: 1, batch: 90, loss: 0.67903, accu: 0.65469, speed: 2.43 step/s\n",
      "global step 100, epoch: 1, batch: 100, loss: 0.48637, accu: 0.65969, speed: 2.34 step/s\n",
      "eval loss: 0.61626, accu: 0.69128\n",
      "global step 110, epoch: 1, batch: 110, loss: 0.65411, accu: 0.70625, speed: 1.02 step/s\n",
      "global step 120, epoch: 1, batch: 120, loss: 0.57731, accu: 0.71719, speed: 2.14 step/s\n",
      "global step 130, epoch: 1, batch: 130, loss: 0.60427, accu: 0.71823, speed: 2.03 step/s\n",
      "global step 140, epoch: 2, batch: 6, loss: 0.61780, accu: 0.71045, speed: 2.47 step/s\n",
      "global step 150, epoch: 2, batch: 16, loss: 0.67262, accu: 0.71244, speed: 2.34 step/s\n",
      "global step 160, epoch: 2, batch: 26, loss: 0.58866, accu: 0.70223, speed: 2.47 step/s\n",
      "global step 170, epoch: 2, batch: 36, loss: 0.60149, accu: 0.69787, speed: 2.34 step/s\n",
      "global step 180, epoch: 2, batch: 46, loss: 0.66451, accu: 0.69166, speed: 2.25 step/s\n",
      "global step 190, epoch: 2, batch: 56, loss: 0.56241, accu: 0.68509, speed: 2.30 step/s\n",
      "global step 200, epoch: 2, batch: 66, loss: 0.68517, accu: 0.68753, speed: 2.40 step/s\n",
      "eval loss: 0.61518, accu: 0.69128\n",
      "global step 210, epoch: 2, batch: 76, loss: 0.59516, accu: 0.69219, speed: 1.03 step/s\n",
      "global step 220, epoch: 2, batch: 86, loss: 0.66069, accu: 0.70703, speed: 2.32 step/s\n",
      "global step 230, epoch: 2, batch: 96, loss: 0.60967, accu: 0.70625, speed: 2.07 step/s\n",
      "global step 240, epoch: 2, batch: 106, loss: 0.62235, accu: 0.70937, speed: 2.32 step/s\n",
      "global step 250, epoch: 2, batch: 116, loss: 0.63534, accu: 0.70750, speed: 2.14 step/s\n",
      "global step 260, epoch: 2, batch: 126, loss: 0.60680, accu: 0.70417, speed: 2.22 step/s\n",
      "global step 270, epoch: 3, batch: 2, loss: 0.61136, accu: 0.70370, speed: 2.44 step/s\n",
      "global step 280, epoch: 3, batch: 12, loss: 0.63492, accu: 0.70618, speed: 2.34 step/s\n",
      "global step 290, epoch: 3, batch: 22, loss: 0.55462, accu: 0.70357, speed: 2.44 step/s\n",
      "global step 300, epoch: 3, batch: 32, loss: 0.57688, accu: 0.70573, speed: 2.36 step/s\n",
      "eval loss: 0.61513, accu: 0.69128\n",
      "global step 310, epoch: 3, batch: 42, loss: 0.57789, accu: 0.70781, speed: 1.10 step/s\n",
      "global step 320, epoch: 3, batch: 52, loss: 0.66882, accu: 0.69141, speed: 2.31 step/s\n",
      "global step 330, epoch: 3, batch: 62, loss: 0.82737, accu: 0.69896, speed: 2.20 step/s\n",
      "global step 340, epoch: 3, batch: 72, loss: 0.65966, accu: 0.69805, speed: 2.06 step/s\n",
      "global step 350, epoch: 3, batch: 82, loss: 0.50436, accu: 0.69719, speed: 2.36 step/s\n",
      "global step 360, epoch: 3, batch: 92, loss: 0.62312, accu: 0.69766, speed: 2.56 step/s\n",
      "global step 370, epoch: 3, batch: 102, loss: 0.55548, accu: 0.70000, speed: 2.41 step/s\n",
      "global step 380, epoch: 3, batch: 112, loss: 0.58284, accu: 0.70195, speed: 2.19 step/s\n",
      "global step 390, epoch: 3, batch: 122, loss: 0.62267, accu: 0.70295, speed: 2.31 step/s\n",
      "global step 400, epoch: 3, batch: 132, loss: 0.53675, accu: 0.70250, speed: 2.43 step/s\n",
      "eval loss: 0.61331, accu: 0.69128\n"
     ]
    }
   ],
   "source": [
    "do_train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47397264-5ac5-49f8-b61d-1f951149abd9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
