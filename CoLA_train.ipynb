{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a2b65876-a093-472b-b6e4-bf0dc578be1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys\n",
    "import paddle\n",
    "sys.path.append('/workspace/fnet_paddle/PaddleNLP')\n",
    "from paddlenlp.datasets import load_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a054e2f7-b649-4323-a237-da478d0982d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:paddle.utils.download:unique_endpoints {''}\n"
     ]
    }
   ],
   "source": [
    "train_ds, dev_ds = load_dataset(\"glue\", name=\"cola\", splits=(\"train\", \"dev\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "33540c1f-1d78-49e0-b971-4e20b2778d74",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([{'sentence': \"Our friends won't buy this analysis, let alone the next one we propose.\",\n",
       "   'labels': 1},\n",
       "  {'sentence': \"One more pseudo generalization and I'm giving up.\",\n",
       "   'labels': 1},\n",
       "  {'sentence': \"One more pseudo generalization or I'm giving up.\",\n",
       "   'labels': 1}],\n",
       " [{'sentence': 'The sailors rode the breeze clear of the rocks.', 'labels': 1},\n",
       "  {'sentence': 'The weights made the rope stretch over the pulley.',\n",
       "   'labels': 1},\n",
       "  {'sentence': 'The mechanical doll wriggled itself loose.', 'labels': 1}])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_ds[:3], dev_ds[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e2f73639-d62f-4f05-9a13-096103980c5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_example(example,\n",
    "                    tokenizer,\n",
    "                    max_seq_length=512,\n",
    "                    is_test=False):\n",
    "    text = example[\"sentence\"]\n",
    "    text_pair = None\n",
    "    encoded_inputs = tokenizer(\n",
    "        text=text, text_pair=text_pair, max_seq_len=max_seq_length)\n",
    "    input_ids = encoded_inputs[\"input_ids\"]\n",
    "    token_type_ids = encoded_inputs[\"token_type_ids\"]\n",
    "\n",
    "    if is_test:\n",
    "        return input_ids, token_type_ids\n",
    "    label = np.array([example[\"labels\"]], dtype=\"int64\")\n",
    "    return input_ids, token_type_ids, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "14e4df59-fb33-4122-8358-f7e15137cd18",
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "import argparse\n",
    "import os\n",
    "import random\n",
    "import time\n",
    "import distutils.util\n",
    "import numpy as np\n",
    "import paddle\n",
    "import paddle.nn.functional as F\n",
    "import paddlenlp as ppnlp\n",
    "from paddlenlp.data import Stack, Tuple, Pad\n",
    "from paddlenlp.datasets import load_dataset\n",
    "from paddlenlp.transformers import LinearDecayWithWarmup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1f639139-6ce0-4b72-ad93-8aca3b3ff84a",
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument(\"--save_dir\", default='./checkpoint', type=str, help=\"The output directory where the model checkpoints will be written.\")\n",
    "parser.add_argument(\"--max_seq_length\", default=128, type=int, help=\"The maximum total input sequence length after tokenization. \"\n",
    "    \"Sequences longer than this will be truncated, sequences shorter will be padded.\")\n",
    "parser.add_argument(\"--batch_size\", default=32, type=int, help=\"Batch size per GPU/CPU for training.\")\n",
    "parser.add_argument(\"--learning_rate\", default=5e-5, type=float, help=\"The initial learning rate for Adam.\")\n",
    "parser.add_argument(\"--weight_decay\", default=0.0, type=float, help=\"Weight decay if we apply some.\")\n",
    "parser.add_argument(\"--epochs\", default=3, type=int, help=\"Total number of training epochs to perform.\")\n",
    "parser.add_argument(\"--warmup_proportion\", default=0.0, type=float, help=\"Linear warmup proption over the training process.\")\n",
    "parser.add_argument(\"--valid_steps\", default=100, type=int, help=\"The interval steps to evaluate model performance.\")\n",
    "parser.add_argument(\"--save_steps\", default=100, type=int, help=\"The interval steps to save checkppoints.\")\n",
    "parser.add_argument(\"--logging_steps\", default=10, type=int, help=\"The interval steps to logging.\")\n",
    "parser.add_argument(\"--init_from_ckpt\", type=str, default=None, help=\"The path of checkpoint to be loaded.\")\n",
    "parser.add_argument(\"--seed\", type=int, default=1000, help=\"random seed for initialization\")\n",
    "parser.add_argument('--device', choices=['cpu', 'gpu', 'xpu', 'npu'], default=\"gpu\", help=\"Select which device to train model, defaults to gpu.\")\n",
    "parser.add_argument(\"--use_amp\", type=distutils.util.strtobool, default=False, help=\"Enable mixed precision training.\")\n",
    "parser.add_argument(\"--scale_loss\", type=float, default=2**15, help=\"The value of scale_loss for fp16.\")\n",
    "args = parser.parse_args([])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cb89ede1-4449-4e9c-8ce4-7802c9d1507f",
   "metadata": {},
   "outputs": [],
   "source": [
    "args.seed = 999\n",
    "args.batch_size = 64\n",
    "args.epochs = 10\n",
    "args.learning_rate=2.5e-03"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b439e14d-68ca-46d9-b529-cc3caa2d27f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_seed(seed):\n",
    "    \"\"\"sets random seed\"\"\"\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    paddle.seed(seed)\n",
    "\n",
    "\n",
    "@paddle.no_grad()\n",
    "def evaluate(model, criterion, metric, data_loader):\n",
    "    \"\"\"\n",
    "    Given a dataset, it evals model and computes the metric.\n",
    "    Args:\n",
    "        model(obj:`paddle.nn.Layer`): A model to classify texts.\n",
    "        data_loader(obj:`paddle.io.DataLoader`): The dataset loader which generates batches.\n",
    "        criterion(obj:`paddle.nn.Layer`): It can compute the loss.\n",
    "        metric(obj:`paddle.metric.Metric`): The evaluation metric.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    metric.reset()\n",
    "    losses = []\n",
    "    for batch in data_loader:\n",
    "        input_ids, token_type_ids, labels = batch\n",
    "        logits = model(input_ids, token_type_ids)\n",
    "        loss = criterion(logits, labels)\n",
    "        losses.append(loss.numpy())\n",
    "        correct = metric.compute(logits, labels)\n",
    "        metric.update(correct)\n",
    "    accu = metric.accumulate()\n",
    "    print(\"eval loss: %.5f, accu: %.5f\" % (np.mean(losses), accu))\n",
    "    model.train()\n",
    "    metric.reset()\n",
    "\n",
    "\n",
    "def create_dataloader(dataset,\n",
    "                      mode='train',\n",
    "                      batch_size=1,\n",
    "                      batchify_fn=None,\n",
    "                      trans_fn=None):\n",
    "    if trans_fn:\n",
    "        dataset = dataset.map(trans_fn)\n",
    "\n",
    "    shuffle = True if mode == 'train' else False\n",
    "    if mode == 'train':\n",
    "        batch_sampler = paddle.io.DistributedBatchSampler(\n",
    "            dataset, batch_size=batch_size, shuffle=shuffle)\n",
    "    else:\n",
    "        batch_sampler = paddle.io.BatchSampler(\n",
    "            dataset, batch_size=batch_size, shuffle=shuffle)\n",
    "\n",
    "    return paddle.io.DataLoader(\n",
    "        dataset=dataset,\n",
    "        batch_sampler=batch_sampler,\n",
    "        collate_fn=batchify_fn,\n",
    "        return_list=True)\n",
    "\n",
    "\n",
    "def do_train():\n",
    "    paddle.set_device(args.device)\n",
    "    rank = paddle.distributed.get_rank()\n",
    "    if paddle.distributed.get_world_size() > 1:\n",
    "        paddle.distributed.init_parallel_env()\n",
    "\n",
    "    set_seed(args.seed)\n",
    "\n",
    "    train_ds, dev_ds = load_dataset(\"glue\", name=\"cola\", splits=(\"train\", \"dev\"))\n",
    "\n",
    "    fnet = ppnlp.transformers.FNetModel.from_pretrained('pretrained_model/paddle/large')\n",
    "    model = ppnlp.transformers.FNetForSequenceClassification(fnet, num_classes=len(train_ds.label_list))\n",
    "    tokenizer = ppnlp.transformers.FNetTokenizer.from_pretrained('pretrained_model/paddle/large')\n",
    "\n",
    "    trans_func = partial(\n",
    "        convert_example,\n",
    "        tokenizer=tokenizer,\n",
    "        max_seq_length=args.max_seq_length)\n",
    "    batchify_fn = lambda samples, fn=Tuple(\n",
    "        Pad(axis=0, pad_val=tokenizer.pad_token_id),  # input\n",
    "        Pad(axis=0, pad_val=tokenizer.pad_token_type_id),  # segment\n",
    "        Stack(dtype=\"int64\")  # label\n",
    "    ): [data for data in fn(samples)]\n",
    "    train_data_loader = create_dataloader(\n",
    "        train_ds,\n",
    "        mode='train',\n",
    "        batch_size=args.batch_size,\n",
    "        batchify_fn=batchify_fn,\n",
    "        trans_fn=trans_func)\n",
    "    dev_data_loader = create_dataloader(\n",
    "        dev_ds,\n",
    "        mode='dev',\n",
    "        batch_size=args.batch_size,\n",
    "        batchify_fn=batchify_fn,\n",
    "        trans_fn=trans_func)\n",
    "\n",
    "    if args.init_from_ckpt and os.path.isfile(args.init_from_ckpt):\n",
    "        state_dict = paddle.load(args.init_from_ckpt)\n",
    "        model.set_dict(state_dict)\n",
    "    model = paddle.DataParallel(model)\n",
    "\n",
    "    num_training_steps = len(train_data_loader) * args.epochs\n",
    "\n",
    "    lr_scheduler = LinearDecayWithWarmup(args.learning_rate, num_training_steps,\n",
    "                                         args.warmup_proportion)\n",
    "\n",
    "    # Generate parameter names needed to perform weight decay.\n",
    "    # All bias and LayerNorm parameters are excluded.\n",
    "    decay_params = [\n",
    "        p.name for n, p in model.named_parameters()\n",
    "        if not any(nd in n for nd in [\"bias\", \"norm\"])\n",
    "    ]\n",
    "    optimizer = paddle.optimizer.AdamW(\n",
    "        learning_rate=lr_scheduler,\n",
    "        parameters=model.parameters(),\n",
    "        weight_decay=args.weight_decay,\n",
    "        apply_decay_param_fun=lambda x: x in decay_params)\n",
    "\n",
    "    criterion = paddle.nn.loss.CrossEntropyLoss()\n",
    "    metric = paddle.metric.Accuracy()\n",
    "    if args.use_amp:\n",
    "        scaler = paddle.amp.GradScaler(init_loss_scaling=args.scale_loss)\n",
    "    global_step = 0\n",
    "    tic_train = time.time()\n",
    "    for epoch in range(1, args.epochs + 1):\n",
    "        for step, batch in enumerate(train_data_loader, start=1):\n",
    "            input_ids, token_type_ids, labels = batch\n",
    "            with paddle.amp.auto_cast(\n",
    "                    args.use_amp,\n",
    "                    custom_white_list=[\"layer_norm\", \"softmax\", \"gelu\"], ):\n",
    "                logits = model(input_ids, token_type_ids)\n",
    "                loss = criterion(logits, labels)\n",
    "            probs = F.softmax(logits, axis=1)\n",
    "            correct = metric.compute(probs, labels)\n",
    "            metric.update(correct)\n",
    "            acc = metric.accumulate()\n",
    "\n",
    "            global_step += 1\n",
    "            if global_step % args.logging_steps == 0 and rank == 0:\n",
    "                print(\n",
    "                    \"global step %d, epoch: %d, batch: %d, loss: %.5f, accu: %.5f, speed: %.2f step/s\"\n",
    "                    % (global_step, epoch, step, loss, acc,\n",
    "                       10 / (time.time() - tic_train)))\n",
    "                tic_train = time.time()\n",
    "            if args.use_amp:\n",
    "                scaler.scale(loss).backward()\n",
    "                scaler.minimize(optimizer, loss)\n",
    "            else:\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "            lr_scheduler.step()\n",
    "            optimizer.clear_grad()\n",
    "\n",
    "            if global_step % args.valid_steps == 0 and rank == 0:\n",
    "                evaluate(model, criterion, metric, dev_data_loader)\n",
    "\n",
    "            if global_step % args.save_steps == 0 and rank == 0:\n",
    "                save_dir = os.path.join(args.save_dir, \"model_%d\" % global_step)\n",
    "                if not os.path.exists(save_dir):\n",
    "                    os.makedirs(save_dir)\n",
    "                model._layers.save_pretrained(save_dir)\n",
    "                tokenizer.save_pretrained(save_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "12c441f5-660e-4f93-83d2-bd3cc20e41c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:paddle.utils.download:unique_endpoints {''}\n",
      "W1127 22:02:54.515415  4681 device_context.cc:447] Please NOTE: device: 0, GPU Compute Capability: 6.0, Driver API Version: 11.0, Runtime API Version: 11.0\n",
      "W1127 22:02:54.527690  4681 device_context.cc:465] device: 0, cuDNN Version: 8.0.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "global step 10, epoch: 1, batch: 10, loss: 1.01809, accu: 0.48906, speed: 2.40 step/s\n",
      "global step 20, epoch: 1, batch: 20, loss: 0.66798, accu: 0.56875, speed: 2.15 step/s\n",
      "global step 30, epoch: 1, batch: 30, loss: 0.65828, accu: 0.59948, speed: 2.42 step/s\n",
      "global step 40, epoch: 1, batch: 40, loss: 0.74000, accu: 0.60898, speed: 2.38 step/s\n",
      "global step 50, epoch: 1, batch: 50, loss: 0.59707, accu: 0.62844, speed: 2.49 step/s\n",
      "global step 60, epoch: 1, batch: 60, loss: 0.83932, accu: 0.63255, speed: 2.14 step/s\n",
      "global step 70, epoch: 1, batch: 70, loss: 0.53087, accu: 0.63996, speed: 2.27 step/s\n",
      "global step 80, epoch: 1, batch: 80, loss: 0.62490, accu: 0.64824, speed: 2.36 step/s\n",
      "global step 90, epoch: 1, batch: 90, loss: 0.68712, accu: 0.65608, speed: 2.44 step/s\n",
      "global step 100, epoch: 1, batch: 100, loss: 0.49179, accu: 0.66125, speed: 2.35 step/s\n",
      "eval loss: 0.61635, accu: 0.69128\n",
      "global step 110, epoch: 1, batch: 110, loss: 0.64313, accu: 0.70625, speed: 0.65 step/s\n",
      "global step 120, epoch: 1, batch: 120, loss: 0.56679, accu: 0.71719, speed: 2.20 step/s\n",
      "global step 130, epoch: 1, batch: 130, loss: 0.63259, accu: 0.71823, speed: 2.10 step/s\n",
      "global step 140, epoch: 2, batch: 6, loss: 0.64150, accu: 0.71045, speed: 2.50 step/s\n",
      "global step 150, epoch: 2, batch: 16, loss: 0.65483, accu: 0.70614, speed: 2.34 step/s\n",
      "global step 160, epoch: 2, batch: 26, loss: 0.55910, accu: 0.69410, speed: 2.47 step/s\n",
      "global step 170, epoch: 2, batch: 36, loss: 0.70836, accu: 0.68934, speed: 2.34 step/s\n",
      "global step 180, epoch: 2, batch: 46, loss: 0.65300, accu: 0.67694, speed: 2.27 step/s\n",
      "global step 190, epoch: 2, batch: 56, loss: 0.52098, accu: 0.67358, speed: 2.27 step/s\n",
      "global step 200, epoch: 2, batch: 66, loss: 0.65236, accu: 0.67780, speed: 2.37 step/s\n",
      "eval loss: 0.66318, accu: 0.69128\n",
      "global step 210, epoch: 2, batch: 76, loss: 0.60289, accu: 0.68750, speed: 0.63 step/s\n",
      "global step 220, epoch: 2, batch: 86, loss: 0.67764, accu: 0.70078, speed: 2.13 step/s\n",
      "global step 230, epoch: 2, batch: 96, loss: 0.60994, accu: 0.70104, speed: 2.00 step/s\n",
      "global step 240, epoch: 2, batch: 106, loss: 0.61818, accu: 0.70547, speed: 2.30 step/s\n",
      "global step 250, epoch: 2, batch: 116, loss: 0.62706, accu: 0.70469, speed: 2.15 step/s\n",
      "global step 260, epoch: 2, batch: 126, loss: 0.61810, accu: 0.69922, speed: 2.25 step/s\n",
      "global step 270, epoch: 3, batch: 2, loss: 0.70334, accu: 0.69652, speed: 2.44 step/s\n",
      "global step 280, epoch: 3, batch: 12, loss: 0.62395, accu: 0.69931, speed: 2.35 step/s\n",
      "global step 290, epoch: 3, batch: 22, loss: 0.61557, accu: 0.69398, speed: 2.48 step/s\n",
      "global step 300, epoch: 3, batch: 32, loss: 0.64416, accu: 0.69475, speed: 2.36 step/s\n",
      "eval loss: 0.75027, accu: 0.69128\n",
      "global step 310, epoch: 3, batch: 42, loss: 0.58319, accu: 0.67500, speed: 0.65 step/s\n",
      "global step 320, epoch: 3, batch: 52, loss: 0.77247, accu: 0.65703, speed: 2.31 step/s\n",
      "global step 330, epoch: 3, batch: 62, loss: 1.17649, accu: 0.66406, speed: 2.27 step/s\n",
      "global step 340, epoch: 3, batch: 72, loss: 0.75544, accu: 0.64687, speed: 2.07 step/s\n",
      "global step 350, epoch: 3, batch: 82, loss: 0.56500, accu: 0.63969, speed: 2.36 step/s\n",
      "global step 360, epoch: 3, batch: 92, loss: 0.62469, accu: 0.64219, speed: 2.54 step/s\n",
      "global step 370, epoch: 3, batch: 102, loss: 0.56448, accu: 0.65246, speed: 2.41 step/s\n",
      "global step 380, epoch: 3, batch: 112, loss: 0.58965, accu: 0.66035, speed: 2.19 step/s\n",
      "global step 390, epoch: 3, batch: 122, loss: 0.62684, accu: 0.66597, speed: 2.32 step/s\n",
      "global step 400, epoch: 3, batch: 132, loss: 0.56705, accu: 0.66922, speed: 2.41 step/s\n",
      "eval loss: 0.61436, accu: 0.69128\n",
      "global step 410, epoch: 4, batch: 8, loss: 0.52392, accu: 0.70894, speed: 0.66 step/s\n",
      "global step 420, epoch: 4, batch: 18, loss: 0.64560, accu: 0.71952, speed: 2.34 step/s\n",
      "global step 430, epoch: 4, batch: 28, loss: 0.56134, accu: 0.71979, speed: 2.37 step/s\n",
      "global step 440, epoch: 4, batch: 38, loss: 0.55718, accu: 0.71755, speed: 2.27 step/s\n",
      "global step 450, epoch: 4, batch: 48, loss: 0.65521, accu: 0.71150, speed: 2.25 step/s\n",
      "global step 460, epoch: 4, batch: 58, loss: 0.67552, accu: 0.70066, speed: 2.38 step/s\n",
      "global step 470, epoch: 4, batch: 68, loss: 0.58899, accu: 0.68934, speed: 2.22 step/s\n",
      "global step 480, epoch: 4, batch: 78, loss: 0.65218, accu: 0.69068, speed: 2.28 step/s\n",
      "global step 490, epoch: 4, batch: 88, loss: 0.62484, accu: 0.69050, speed: 2.20 step/s\n",
      "global step 500, epoch: 4, batch: 98, loss: 0.83040, accu: 0.69098, speed: 2.12 step/s\n",
      "eval loss: 0.61304, accu: 0.69128\n",
      "global step 510, epoch: 4, batch: 108, loss: 0.60725, accu: 0.71562, speed: 0.65 step/s\n",
      "global step 520, epoch: 4, batch: 118, loss: 0.65952, accu: 0.71016, speed: 2.35 step/s\n",
      "global step 530, epoch: 4, batch: 128, loss: 0.79695, accu: 0.70521, speed: 2.27 step/s\n",
      "global step 540, epoch: 5, batch: 4, loss: 0.59061, accu: 0.71045, speed: 2.34 step/s\n",
      "global step 550, epoch: 5, batch: 14, loss: 0.68710, accu: 0.71276, speed: 2.46 step/s\n",
      "global step 560, epoch: 5, batch: 24, loss: 0.52049, accu: 0.70878, speed: 2.23 step/s\n",
      "global step 570, epoch: 5, batch: 34, loss: 0.58661, accu: 0.71044, speed: 2.21 step/s\n",
      "global step 580, epoch: 5, batch: 44, loss: 0.62632, accu: 0.70618, speed: 2.30 step/s\n",
      "global step 590, epoch: 5, batch: 54, loss: 0.67021, accu: 0.70811, speed: 2.22 step/s\n",
      "global step 600, epoch: 5, batch: 64, loss: 0.57947, accu: 0.70306, speed: 2.42 step/s\n",
      "eval loss: 0.63607, accu: 0.69128\n",
      "global step 610, epoch: 5, batch: 74, loss: 0.63318, accu: 0.70937, speed: 0.64 step/s\n",
      "global step 620, epoch: 5, batch: 84, loss: 0.57794, accu: 0.70391, speed: 2.26 step/s\n",
      "global step 630, epoch: 5, batch: 94, loss: 0.58931, accu: 0.70937, speed: 2.27 step/s\n",
      "global step 640, epoch: 5, batch: 104, loss: 0.59186, accu: 0.70859, speed: 2.15 step/s\n",
      "global step 650, epoch: 5, batch: 114, loss: 0.58138, accu: 0.70937, speed: 2.32 step/s\n",
      "global step 660, epoch: 5, batch: 124, loss: 0.61282, accu: 0.70573, speed: 2.39 step/s\n",
      "global step 670, epoch: 5, batch: 134, loss: 0.58572, accu: 0.70348, speed: 2.16 step/s\n",
      "global step 680, epoch: 6, batch: 10, loss: 0.64176, accu: 0.70422, speed: 2.19 step/s\n",
      "global step 690, epoch: 6, batch: 20, loss: 0.65456, accu: 0.70357, speed: 2.34 step/s\n",
      "global step 700, epoch: 6, batch: 30, loss: 0.64772, accu: 0.70541, speed: 2.35 step/s\n",
      "eval loss: 0.61384, accu: 0.69128\n",
      "global step 710, epoch: 6, batch: 40, loss: 0.64132, accu: 0.70937, speed: 0.65 step/s\n",
      "global step 720, epoch: 6, batch: 50, loss: 0.66063, accu: 0.71562, speed: 2.14 step/s\n",
      "global step 730, epoch: 6, batch: 60, loss: 0.64082, accu: 0.69219, speed: 2.37 step/s\n",
      "global step 740, epoch: 6, batch: 70, loss: 0.63083, accu: 0.69844, speed: 2.36 step/s\n",
      "global step 750, epoch: 6, batch: 80, loss: 0.58552, accu: 0.69437, speed: 2.28 step/s\n",
      "global step 760, epoch: 6, batch: 90, loss: 0.63646, accu: 0.69401, speed: 2.35 step/s\n",
      "global step 770, epoch: 6, batch: 100, loss: 0.65635, accu: 0.69799, speed: 2.25 step/s\n",
      "global step 780, epoch: 6, batch: 110, loss: 0.52648, accu: 0.70117, speed: 2.37 step/s\n",
      "global step 790, epoch: 6, batch: 120, loss: 0.65660, accu: 0.69948, speed: 2.27 step/s\n",
      "global step 800, epoch: 6, batch: 130, loss: 0.60486, accu: 0.70031, speed: 2.40 step/s\n",
      "eval loss: 0.61397, accu: 0.69128\n",
      "global step 810, epoch: 7, batch: 6, loss: 0.61680, accu: 0.72683, speed: 0.69 step/s\n",
      "global step 820, epoch: 7, batch: 16, loss: 0.53565, accu: 0.72351, speed: 2.27 step/s\n",
      "global step 830, epoch: 7, batch: 26, loss: 0.59626, accu: 0.71715, speed: 2.15 step/s\n",
      "global step 840, epoch: 7, batch: 36, loss: 0.64725, accu: 0.71282, speed: 2.40 step/s\n",
      "global step 850, epoch: 7, batch: 46, loss: 0.55262, accu: 0.70677, speed: 2.46 step/s\n",
      "global step 860, epoch: 7, batch: 56, loss: 0.64752, accu: 0.71035, speed: 2.13 step/s\n",
      "global step 870, epoch: 7, batch: 66, loss: 0.61858, accu: 0.71268, speed: 2.29 step/s\n",
      "global step 880, epoch: 7, batch: 76, loss: 0.66953, accu: 0.70638, speed: 2.28 step/s\n",
      "global step 890, epoch: 7, batch: 86, loss: 0.66486, accu: 0.70724, speed: 2.21 step/s\n",
      "global step 900, epoch: 7, batch: 96, loss: 0.64735, accu: 0.70682, speed: 2.04 step/s\n",
      "eval loss: 0.62250, accu: 0.69128\n",
      "global step 910, epoch: 7, batch: 106, loss: 0.57985, accu: 0.72344, speed: 1.02 step/s\n",
      "global step 920, epoch: 7, batch: 116, loss: 0.66229, accu: 0.70156, speed: 2.14 step/s\n",
      "global step 930, epoch: 7, batch: 126, loss: 0.63364, accu: 0.69323, speed: 2.23 step/s\n",
      "global step 940, epoch: 8, batch: 2, loss: 0.67433, accu: 0.69704, speed: 2.62 step/s\n",
      "global step 950, epoch: 8, batch: 12, loss: 0.60534, accu: 0.70362, speed: 2.19 step/s\n",
      "global step 960, epoch: 8, batch: 22, loss: 0.55113, accu: 0.70773, speed: 2.38 step/s\n",
      "global step 970, epoch: 8, batch: 32, loss: 0.68413, accu: 0.70438, speed: 2.56 step/s\n",
      "global step 980, epoch: 8, batch: 42, loss: 0.70642, accu: 0.70304, speed: 2.20 step/s\n",
      "global step 990, epoch: 8, batch: 52, loss: 0.55870, accu: 0.70392, speed: 2.36 step/s\n",
      "global step 1000, epoch: 8, batch: 62, loss: 0.64301, accu: 0.70243, speed: 2.40 step/s\n",
      "eval loss: 0.61367, accu: 0.69128\n",
      "global step 1010, epoch: 8, batch: 72, loss: 0.63135, accu: 0.67500, speed: 1.03 step/s\n",
      "global step 1020, epoch: 8, batch: 82, loss: 0.53698, accu: 0.70156, speed: 2.40 step/s\n",
      "global step 1030, epoch: 8, batch: 92, loss: 0.58310, accu: 0.70625, speed: 2.28 step/s\n",
      "global step 1040, epoch: 8, batch: 102, loss: 0.61892, accu: 0.70039, speed: 2.22 step/s\n",
      "global step 1050, epoch: 8, batch: 112, loss: 0.55518, accu: 0.70063, speed: 2.19 step/s\n",
      "global step 1060, epoch: 8, batch: 122, loss: 0.68516, accu: 0.69896, speed: 2.34 step/s\n",
      "global step 1070, epoch: 8, batch: 132, loss: 0.68182, accu: 0.70357, speed: 2.42 step/s\n",
      "global step 1080, epoch: 9, batch: 8, loss: 0.57285, accu: 0.70147, speed: 2.31 step/s\n",
      "global step 1090, epoch: 9, batch: 18, loss: 0.57595, accu: 0.70323, speed: 2.09 step/s\n",
      "global step 1100, epoch: 9, batch: 28, loss: 0.57792, accu: 0.70055, speed: 2.17 step/s\n",
      "eval loss: 0.62327, accu: 0.69128\n",
      "global step 1110, epoch: 9, batch: 38, loss: 0.52716, accu: 0.71094, speed: 1.09 step/s\n",
      "global step 1120, epoch: 9, batch: 48, loss: 0.55555, accu: 0.72578, speed: 2.42 step/s\n",
      "global step 1130, epoch: 9, batch: 58, loss: 0.58524, accu: 0.72240, speed: 2.31 step/s\n",
      "global step 1140, epoch: 9, batch: 68, loss: 0.61799, accu: 0.71836, speed: 2.32 step/s\n",
      "global step 1150, epoch: 9, batch: 78, loss: 0.61092, accu: 0.71406, speed: 2.37 step/s\n",
      "global step 1160, epoch: 9, batch: 88, loss: 0.62076, accu: 0.71016, speed: 2.31 step/s\n",
      "global step 1170, epoch: 9, batch: 98, loss: 0.61371, accu: 0.70670, speed: 2.33 step/s\n",
      "global step 1180, epoch: 9, batch: 108, loss: 0.68632, accu: 0.70566, speed: 2.20 step/s\n",
      "global step 1190, epoch: 9, batch: 118, loss: 0.65084, accu: 0.70417, speed: 2.45 step/s\n",
      "global step 1200, epoch: 9, batch: 128, loss: 0.59065, accu: 0.70422, speed: 2.43 step/s\n",
      "eval loss: 0.61662, accu: 0.69128\n",
      "global step 1210, epoch: 10, batch: 4, loss: 0.58725, accu: 0.73171, speed: 1.05 step/s\n",
      "global step 1220, epoch: 10, batch: 14, loss: 0.65537, accu: 0.70279, speed: 2.33 step/s\n",
      "global step 1230, epoch: 10, batch: 24, loss: 0.60772, accu: 0.69868, speed: 2.41 step/s\n",
      "global step 1240, epoch: 10, batch: 34, loss: 0.57455, accu: 0.70966, speed: 2.38 step/s\n",
      "global step 1250, epoch: 10, batch: 44, loss: 0.64603, accu: 0.70835, speed: 2.35 step/s\n",
      "global step 1260, epoch: 10, batch: 54, loss: 0.56314, accu: 0.70878, speed: 2.32 step/s\n",
      "global step 1270, epoch: 10, batch: 64, loss: 0.62938, accu: 0.70864, speed: 2.17 step/s\n",
      "global step 1280, epoch: 10, batch: 74, loss: 0.59663, accu: 0.70775, speed: 2.39 step/s\n",
      "global step 1290, epoch: 10, batch: 84, loss: 0.68889, accu: 0.70759, speed: 2.23 step/s\n",
      "global step 1300, epoch: 10, batch: 94, loss: 0.65178, accu: 0.70478, speed: 2.25 step/s\n",
      "eval loss: 0.61376, accu: 0.69128\n",
      "global step 1310, epoch: 10, batch: 104, loss: 0.53401, accu: 0.73281, speed: 1.05 step/s\n",
      "global step 1320, epoch: 10, batch: 114, loss: 0.58436, accu: 0.71875, speed: 2.25 step/s\n",
      "global step 1330, epoch: 10, batch: 124, loss: 0.65494, accu: 0.70990, speed: 2.37 step/s\n",
      "global step 1340, epoch: 10, batch: 134, loss: 0.62738, accu: 0.70888, speed: 2.13 step/s\n"
     ]
    }
   ],
   "source": [
    "do_train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47397264-5ac5-49f8-b61d-1f951149abd9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
