{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a2b65876-a093-472b-b6e4-bf0dc578be1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys\n",
    "import paddle\n",
    "sys.path.append('/workspace/fnet_paddle/PaddleNLP')\n",
    "from paddlenlp.datasets import load_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a054e2f7-b649-4323-a237-da478d0982d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:paddle.utils.download:unique_endpoints {''}\n"
     ]
    }
   ],
   "source": [
    "train_ds, dev_ds = load_dataset(\"glue\", name=\"cola\", splits=(\"train\", \"dev\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "33540c1f-1d78-49e0-b971-4e20b2778d74",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([{'sentence': \"Our friends won't buy this analysis, let alone the next one we propose.\",\n",
       "   'labels': 1},\n",
       "  {'sentence': \"One more pseudo generalization and I'm giving up.\",\n",
       "   'labels': 1},\n",
       "  {'sentence': \"One more pseudo generalization or I'm giving up.\",\n",
       "   'labels': 1}],\n",
       " [{'sentence': 'The sailors rode the breeze clear of the rocks.', 'labels': 1},\n",
       "  {'sentence': 'The weights made the rope stretch over the pulley.',\n",
       "   'labels': 1},\n",
       "  {'sentence': 'The mechanical doll wriggled itself loose.', 'labels': 1}])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_ds[:3], dev_ds[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e2f73639-d62f-4f05-9a13-096103980c5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_example(example,\n",
    "                    tokenizer,\n",
    "                    max_seq_length=512,\n",
    "                    is_test=False):\n",
    "    text = example[\"sentence\"]\n",
    "    text_pair = None\n",
    "    encoded_inputs = tokenizer(\n",
    "        text=text, text_pair=text_pair, max_seq_len=max_seq_length)\n",
    "    input_ids = encoded_inputs[\"input_ids\"]\n",
    "    token_type_ids = encoded_inputs[\"token_type_ids\"]\n",
    "\n",
    "    if is_test:\n",
    "        return input_ids, token_type_ids\n",
    "    label = np.array([example[\"labels\"]], dtype=\"int64\")\n",
    "    return input_ids, token_type_ids, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "14e4df59-fb33-4122-8358-f7e15137cd18",
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "import argparse\n",
    "import os\n",
    "import random\n",
    "import time\n",
    "import distutils.util\n",
    "import numpy as np\n",
    "import paddle\n",
    "import paddle.nn.functional as F\n",
    "import paddlenlp as ppnlp\n",
    "from paddlenlp.data import Stack, Tuple, Pad\n",
    "from paddlenlp.datasets import load_dataset\n",
    "from paddlenlp.transformers import LinearDecayWithWarmup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1f639139-6ce0-4b72-ad93-8aca3b3ff84a",
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument(\"--save_dir\", default='./checkpoint', type=str, help=\"The output directory where the model checkpoints will be written.\")\n",
    "parser.add_argument(\"--max_seq_length\", default=128, type=int, help=\"The maximum total input sequence length after tokenization. \"\n",
    "    \"Sequences longer than this will be truncated, sequences shorter will be padded.\")\n",
    "parser.add_argument(\"--batch_size\", default=32, type=int, help=\"Batch size per GPU/CPU for training.\")\n",
    "parser.add_argument(\"--learning_rate\", default=5e-5, type=float, help=\"The initial learning rate for Adam.\")\n",
    "parser.add_argument(\"--weight_decay\", default=0.0, type=float, help=\"Weight decay if we apply some.\")\n",
    "parser.add_argument(\"--epochs\", default=3, type=int, help=\"Total number of training epochs to perform.\")\n",
    "parser.add_argument(\"--warmup_proportion\", default=0.0, type=float, help=\"Linear warmup proption over the training process.\")\n",
    "parser.add_argument(\"--valid_steps\", default=100, type=int, help=\"The interval steps to evaluate model performance.\")\n",
    "parser.add_argument(\"--save_steps\", default=100, type=int, help=\"The interval steps to save checkppoints.\")\n",
    "parser.add_argument(\"--logging_steps\", default=10, type=int, help=\"The interval steps to logging.\")\n",
    "parser.add_argument(\"--init_from_ckpt\", type=str, default=None, help=\"The path of checkpoint to be loaded.\")\n",
    "parser.add_argument(\"--seed\", type=int, default=1000, help=\"random seed for initialization\")\n",
    "parser.add_argument('--device', choices=['cpu', 'gpu', 'xpu', 'npu'], default=\"gpu\", help=\"Select which device to train model, defaults to gpu.\")\n",
    "parser.add_argument(\"--use_amp\", type=distutils.util.strtobool, default=False, help=\"Enable mixed precision training.\")\n",
    "parser.add_argument(\"--scale_loss\", type=float, default=2**15, help=\"The value of scale_loss for fp16.\")\n",
    "args = parser.parse_args([])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cb89ede1-4449-4e9c-8ce4-7802c9d1507f",
   "metadata": {},
   "outputs": [],
   "source": [
    "args.seed = 999\n",
    "args.batch_size = 64\n",
    "args.epochs = 3\n",
    "args.learning_rate=2.5e-03"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b439e14d-68ca-46d9-b529-cc3caa2d27f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_seed(seed):\n",
    "    \"\"\"sets random seed\"\"\"\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    paddle.seed(seed)\n",
    "\n",
    "\n",
    "@paddle.no_grad()\n",
    "def evaluate(model, criterion, metric, data_loader):\n",
    "    \"\"\"\n",
    "    Given a dataset, it evals model and computes the metric.\n",
    "    Args:\n",
    "        model(obj:`paddle.nn.Layer`): A model to classify texts.\n",
    "        data_loader(obj:`paddle.io.DataLoader`): The dataset loader which generates batches.\n",
    "        criterion(obj:`paddle.nn.Layer`): It can compute the loss.\n",
    "        metric(obj:`paddle.metric.Metric`): The evaluation metric.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    metric.reset()\n",
    "    losses = []\n",
    "    for batch in data_loader:\n",
    "        input_ids, token_type_ids, labels = batch\n",
    "        logits = model(input_ids, token_type_ids)\n",
    "        loss = criterion(logits, labels)\n",
    "        losses.append(loss.numpy())\n",
    "        correct = metric.compute(logits, labels)\n",
    "        metric.update(correct)\n",
    "    accu = metric.accumulate()\n",
    "    print(\"eval loss: %.5f, accu: %.5f\" % (np.mean(losses), accu))\n",
    "    model.train()\n",
    "    metric.reset()\n",
    "\n",
    "\n",
    "def create_dataloader(dataset,\n",
    "                      mode='train',\n",
    "                      batch_size=1,\n",
    "                      batchify_fn=None,\n",
    "                      trans_fn=None):\n",
    "    if trans_fn:\n",
    "        dataset = dataset.map(trans_fn)\n",
    "\n",
    "    shuffle = True if mode == 'train' else False\n",
    "    if mode == 'train':\n",
    "        batch_sampler = paddle.io.DistributedBatchSampler(\n",
    "            dataset, batch_size=batch_size, shuffle=shuffle)\n",
    "    else:\n",
    "        batch_sampler = paddle.io.BatchSampler(\n",
    "            dataset, batch_size=batch_size, shuffle=shuffle)\n",
    "\n",
    "    return paddle.io.DataLoader(\n",
    "        dataset=dataset,\n",
    "        batch_sampler=batch_sampler,\n",
    "        collate_fn=batchify_fn,\n",
    "        return_list=True)\n",
    "\n",
    "\n",
    "def do_train():\n",
    "    paddle.set_device(args.device)\n",
    "    rank = paddle.distributed.get_rank()\n",
    "    if paddle.distributed.get_world_size() > 1:\n",
    "        paddle.distributed.init_parallel_env()\n",
    "\n",
    "    set_seed(args.seed)\n",
    "\n",
    "    train_ds, dev_ds = load_dataset(\"glue\", name=\"cola\", splits=(\"train\", \"dev\"))\n",
    "\n",
    "    fnet = ppnlp.transformers.FNetModel.from_pretrained('pretrained_model/paddle/large')\n",
    "    model = ppnlp.transformers.FNetForSequenceClassification(fnet, num_classes=len(train_ds.label_list))\n",
    "    tokenizer = ppnlp.transformers.FNetTokenizer.from_pretrained('fnet-large')\n",
    "\n",
    "    trans_func = partial(\n",
    "        convert_example,\n",
    "        tokenizer=tokenizer,\n",
    "        max_seq_length=args.max_seq_length)\n",
    "    batchify_fn = lambda samples, fn=Tuple(\n",
    "        Pad(axis=0, pad_val=tokenizer.pad_token_id),  # input\n",
    "        Pad(axis=0, pad_val=tokenizer.pad_token_type_id),  # segment\n",
    "        Stack(dtype=\"int64\")  # label\n",
    "    ): [data for data in fn(samples)]\n",
    "    train_data_loader = create_dataloader(\n",
    "        train_ds,\n",
    "        mode='train',\n",
    "        batch_size=args.batch_size,\n",
    "        batchify_fn=batchify_fn,\n",
    "        trans_fn=trans_func)\n",
    "    dev_data_loader = create_dataloader(\n",
    "        dev_ds,\n",
    "        mode='dev',\n",
    "        batch_size=args.batch_size,\n",
    "        batchify_fn=batchify_fn,\n",
    "        trans_fn=trans_func)\n",
    "\n",
    "    if args.init_from_ckpt and os.path.isfile(args.init_from_ckpt):\n",
    "        state_dict = paddle.load(args.init_from_ckpt)\n",
    "        model.set_dict(state_dict)\n",
    "    model = paddle.DataParallel(model)\n",
    "\n",
    "    num_training_steps = len(train_data_loader) * args.epochs\n",
    "\n",
    "    lr_scheduler = LinearDecayWithWarmup(args.learning_rate, num_training_steps,\n",
    "                                         args.warmup_proportion)\n",
    "\n",
    "    # Generate parameter names needed to perform weight decay.\n",
    "    # All bias and LayerNorm parameters are excluded.\n",
    "    decay_params = [\n",
    "        p.name for n, p in model.named_parameters()\n",
    "        if not any(nd in n for nd in [\"bias\", \"norm\"])\n",
    "    ]\n",
    "    optimizer = paddle.optimizer.AdamW(\n",
    "        learning_rate=lr_scheduler,\n",
    "        parameters=model.parameters(),\n",
    "        weight_decay=args.weight_decay,\n",
    "        apply_decay_param_fun=lambda x: x in decay_params)\n",
    "\n",
    "    criterion = paddle.nn.loss.CrossEntropyLoss()\n",
    "    metric = paddle.metric.Accuracy()\n",
    "    if args.use_amp:\n",
    "        scaler = paddle.amp.GradScaler(init_loss_scaling=args.scale_loss)\n",
    "    global_step = 0\n",
    "    tic_train = time.time()\n",
    "    for epoch in range(1, args.epochs + 1):\n",
    "        for step, batch in enumerate(train_data_loader, start=1):\n",
    "            input_ids, token_type_ids, labels = batch\n",
    "            with paddle.amp.auto_cast(\n",
    "                    args.use_amp,\n",
    "                    custom_white_list=[\"layer_norm\", \"softmax\", \"gelu\"], ):\n",
    "                logits = model(input_ids, token_type_ids)\n",
    "                loss = criterion(logits, labels)\n",
    "            probs = F.softmax(logits, axis=1)\n",
    "            correct = metric.compute(probs, labels)\n",
    "            metric.update(correct)\n",
    "            acc = metric.accumulate()\n",
    "\n",
    "            global_step += 1\n",
    "            if global_step % args.logging_steps == 0 and rank == 0:\n",
    "                print(\n",
    "                    \"global step %d, epoch: %d, batch: %d, loss: %.5f, accu: %.5f, speed: %.2f step/s\"\n",
    "                    % (global_step, epoch, step, loss, acc,\n",
    "                       10 / (time.time() - tic_train)))\n",
    "                tic_train = time.time()\n",
    "            if args.use_amp:\n",
    "                scaler.scale(loss).backward()\n",
    "                scaler.minimize(optimizer, loss)\n",
    "            else:\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "            lr_scheduler.step()\n",
    "            optimizer.clear_grad()\n",
    "\n",
    "            if global_step % args.valid_steps == 0 and rank == 0:\n",
    "                evaluate(model, criterion, metric, dev_data_loader)\n",
    "\n",
    "            if global_step % args.save_steps == 0 and rank == 0:\n",
    "                save_dir = os.path.join(args.save_dir, \"model_%d\" % global_step)\n",
    "                if not os.path.exists(save_dir):\n",
    "                    os.makedirs(save_dir)\n",
    "                model._layers.save_pretrained(save_dir)\n",
    "                tokenizer.save_pretrained(save_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12c441f5-660e-4f93-83d2-bd3cc20e41c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:paddle.utils.download:unique_endpoints {''}\n",
      "\u001b[32m[2021-11-28 00:38:55,918] [    INFO]\u001b[0m - Already cached /root/.paddlenlp/models/fnet-large/spiece.model\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "global step 10, epoch: 1, batch: 10, loss: 1.14025, accu: 0.50000, speed: 2.40 step/s\n",
      "global step 20, epoch: 1, batch: 20, loss: 0.63458, accu: 0.59297, speed: 2.15 step/s\n"
     ]
    }
   ],
   "source": [
    "do_train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47397264-5ac5-49f8-b61d-1f951149abd9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
