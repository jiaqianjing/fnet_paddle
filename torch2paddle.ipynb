{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2cd6aa35-6b9b-4d9f-8b41-4db436a93bd5",
   "metadata": {},
   "source": [
    "## 模型参数 torch 转换 paddle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "54b6e62a-5dd3-41e0-b732-1df49bb5ad06",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "# load transformer 源码\n",
    "sys.path.append('/workspace/transformers/src/')\n",
    "\n",
    "sys.path.append('/workspace/PaddleNLP')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "89906706-e028-4de8-ae6e-c278a7768a1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t:  embeddings.position_ids \t torch.Size([1, 512])\n",
      "p:  embeddings.position_ids \t [1, 512] \n",
      "\n",
      "t:  embeddings.word_embeddings.weight \t torch.Size([32000, 1024])\n",
      "p:  embeddings.word_embeddings.weight \t [32000, 1024] \n",
      "\n",
      "t:  embeddings.position_embeddings.weight \t torch.Size([512, 1024])\n",
      "p:  embeddings.position_embeddings.weight \t [512, 1024] \n",
      "\n",
      "t:  embeddings.token_type_embeddings.weight \t torch.Size([4, 1024])\n",
      "p:  embeddings.token_type_embeddings.weight \t [4, 1024] \n",
      "\n",
      "t:  embeddings.LayerNorm.weight \t torch.Size([1024])\n",
      "p:  embeddings.layer_norm.weight \t [1024] \n",
      "\n",
      "t:  embeddings.LayerNorm.bias \t torch.Size([1024])\n",
      "p:  embeddings.layer_norm.bias \t [1024] \n",
      "\n",
      "transpose(permute) ---------->\n",
      "t:  embeddings.projection.weight \t torch.Size([1024, 1024])\n",
      "p:  embeddings.projection.weight \t [1024, 1024] \n",
      "\n",
      "transpose(permute) ---------->\n",
      "t:  embeddings.projection.bias \t torch.Size([1024])\n",
      "p:  embeddings.projection.bias \t [1024] \n",
      "\n",
      "t:  encoder.layer.0.fourier.output.LayerNorm.weight \t torch.Size([1024])\n",
      "p:  encoder.layers.0.fourier.output.layer_norm.weight \t [1024] \n",
      "\n",
      "t:  encoder.layer.0.fourier.output.LayerNorm.bias \t torch.Size([1024])\n",
      "p:  encoder.layers.0.fourier.output.layer_norm.bias \t [1024] \n",
      "\n",
      "transpose(permute) ---------->\n",
      "t:  encoder.layer.0.intermediate.dense.weight \t torch.Size([4096, 1024])\n",
      "p:  encoder.layers.0.intermediate.dense.weight \t [1024, 4096] \n",
      "\n",
      "t:  encoder.layer.0.intermediate.dense.bias \t torch.Size([4096])\n",
      "p:  encoder.layers.0.intermediate.dense.bias \t [4096] \n",
      "\n",
      "transpose(permute) ---------->\n",
      "t:  encoder.layer.0.output.dense.weight \t torch.Size([1024, 4096])\n",
      "p:  encoder.layers.0.output.dense.weight \t [4096, 1024] \n",
      "\n",
      "t:  encoder.layer.0.output.dense.bias \t torch.Size([1024])\n",
      "p:  encoder.layers.0.output.dense.bias \t [1024] \n",
      "\n",
      "t:  encoder.layer.0.output.LayerNorm.weight \t torch.Size([1024])\n",
      "p:  encoder.layers.0.output.layer_norm.weight \t [1024] \n",
      "\n",
      "t:  encoder.layer.0.output.LayerNorm.bias \t torch.Size([1024])\n",
      "p:  encoder.layers.0.output.layer_norm.bias \t [1024] \n",
      "\n",
      "t:  encoder.layer.1.fourier.output.LayerNorm.weight \t torch.Size([1024])\n",
      "p:  encoder.layers.1.fourier.output.layer_norm.weight \t [1024] \n",
      "\n",
      "t:  encoder.layer.1.fourier.output.LayerNorm.bias \t torch.Size([1024])\n",
      "p:  encoder.layers.1.fourier.output.layer_norm.bias \t [1024] \n",
      "\n",
      "transpose(permute) ---------->\n",
      "t:  encoder.layer.1.intermediate.dense.weight \t torch.Size([4096, 1024])\n",
      "p:  encoder.layers.1.intermediate.dense.weight \t [1024, 4096] \n",
      "\n",
      "t:  encoder.layer.1.intermediate.dense.bias \t torch.Size([4096])\n",
      "p:  encoder.layers.1.intermediate.dense.bias \t [4096] \n",
      "\n",
      "transpose(permute) ---------->\n",
      "t:  encoder.layer.1.output.dense.weight \t torch.Size([1024, 4096])\n",
      "p:  encoder.layers.1.output.dense.weight \t [4096, 1024] \n",
      "\n",
      "t:  encoder.layer.1.output.dense.bias \t torch.Size([1024])\n",
      "p:  encoder.layers.1.output.dense.bias \t [1024] \n",
      "\n",
      "t:  encoder.layer.1.output.LayerNorm.weight \t torch.Size([1024])\n",
      "p:  encoder.layers.1.output.layer_norm.weight \t [1024] \n",
      "\n",
      "t:  encoder.layer.1.output.LayerNorm.bias \t torch.Size([1024])\n",
      "p:  encoder.layers.1.output.layer_norm.bias \t [1024] \n",
      "\n",
      "t:  encoder.layer.2.fourier.output.LayerNorm.weight \t torch.Size([1024])\n",
      "p:  encoder.layers.2.fourier.output.layer_norm.weight \t [1024] \n",
      "\n",
      "t:  encoder.layer.2.fourier.output.LayerNorm.bias \t torch.Size([1024])\n",
      "p:  encoder.layers.2.fourier.output.layer_norm.bias \t [1024] \n",
      "\n",
      "transpose(permute) ---------->\n",
      "t:  encoder.layer.2.intermediate.dense.weight \t torch.Size([4096, 1024])\n",
      "p:  encoder.layers.2.intermediate.dense.weight \t [1024, 4096] \n",
      "\n",
      "t:  encoder.layer.2.intermediate.dense.bias \t torch.Size([4096])\n",
      "p:  encoder.layers.2.intermediate.dense.bias \t [4096] \n",
      "\n",
      "transpose(permute) ---------->\n",
      "t:  encoder.layer.2.output.dense.weight \t torch.Size([1024, 4096])\n",
      "p:  encoder.layers.2.output.dense.weight \t [4096, 1024] \n",
      "\n",
      "t:  encoder.layer.2.output.dense.bias \t torch.Size([1024])\n",
      "p:  encoder.layers.2.output.dense.bias \t [1024] \n",
      "\n",
      "t:  encoder.layer.2.output.LayerNorm.weight \t torch.Size([1024])\n",
      "p:  encoder.layers.2.output.layer_norm.weight \t [1024] \n",
      "\n",
      "t:  encoder.layer.2.output.LayerNorm.bias \t torch.Size([1024])\n",
      "p:  encoder.layers.2.output.layer_norm.bias \t [1024] \n",
      "\n",
      "t:  encoder.layer.3.fourier.output.LayerNorm.weight \t torch.Size([1024])\n",
      "p:  encoder.layers.3.fourier.output.layer_norm.weight \t [1024] \n",
      "\n",
      "t:  encoder.layer.3.fourier.output.LayerNorm.bias \t torch.Size([1024])\n",
      "p:  encoder.layers.3.fourier.output.layer_norm.bias \t [1024] \n",
      "\n",
      "transpose(permute) ---------->\n",
      "t:  encoder.layer.3.intermediate.dense.weight \t torch.Size([4096, 1024])\n",
      "p:  encoder.layers.3.intermediate.dense.weight \t [1024, 4096] \n",
      "\n",
      "t:  encoder.layer.3.intermediate.dense.bias \t torch.Size([4096])\n",
      "p:  encoder.layers.3.intermediate.dense.bias \t [4096] \n",
      "\n",
      "transpose(permute) ---------->\n",
      "t:  encoder.layer.3.output.dense.weight \t torch.Size([1024, 4096])\n",
      "p:  encoder.layers.3.output.dense.weight \t [4096, 1024] \n",
      "\n",
      "t:  encoder.layer.3.output.dense.bias \t torch.Size([1024])\n",
      "p:  encoder.layers.3.output.dense.bias \t [1024] \n",
      "\n",
      "t:  encoder.layer.3.output.LayerNorm.weight \t torch.Size([1024])\n",
      "p:  encoder.layers.3.output.layer_norm.weight \t [1024] \n",
      "\n",
      "t:  encoder.layer.3.output.LayerNorm.bias \t torch.Size([1024])\n",
      "p:  encoder.layers.3.output.layer_norm.bias \t [1024] \n",
      "\n",
      "t:  encoder.layer.4.fourier.output.LayerNorm.weight \t torch.Size([1024])\n",
      "p:  encoder.layers.4.fourier.output.layer_norm.weight \t [1024] \n",
      "\n",
      "t:  encoder.layer.4.fourier.output.LayerNorm.bias \t torch.Size([1024])\n",
      "p:  encoder.layers.4.fourier.output.layer_norm.bias \t [1024] \n",
      "\n",
      "transpose(permute) ---------->\n",
      "t:  encoder.layer.4.intermediate.dense.weight \t torch.Size([4096, 1024])\n",
      "p:  encoder.layers.4.intermediate.dense.weight \t [1024, 4096] \n",
      "\n",
      "t:  encoder.layer.4.intermediate.dense.bias \t torch.Size([4096])\n",
      "p:  encoder.layers.4.intermediate.dense.bias \t [4096] \n",
      "\n",
      "transpose(permute) ---------->\n",
      "t:  encoder.layer.4.output.dense.weight \t torch.Size([1024, 4096])\n",
      "p:  encoder.layers.4.output.dense.weight \t [4096, 1024] \n",
      "\n",
      "t:  encoder.layer.4.output.dense.bias \t torch.Size([1024])\n",
      "p:  encoder.layers.4.output.dense.bias \t [1024] \n",
      "\n",
      "t:  encoder.layer.4.output.LayerNorm.weight \t torch.Size([1024])\n",
      "p:  encoder.layers.4.output.layer_norm.weight \t [1024] \n",
      "\n",
      "t:  encoder.layer.4.output.LayerNorm.bias \t torch.Size([1024])\n",
      "p:  encoder.layers.4.output.layer_norm.bias \t [1024] \n",
      "\n",
      "t:  encoder.layer.5.fourier.output.LayerNorm.weight \t torch.Size([1024])\n",
      "p:  encoder.layers.5.fourier.output.layer_norm.weight \t [1024] \n",
      "\n",
      "t:  encoder.layer.5.fourier.output.LayerNorm.bias \t torch.Size([1024])\n",
      "p:  encoder.layers.5.fourier.output.layer_norm.bias \t [1024] \n",
      "\n",
      "transpose(permute) ---------->\n",
      "t:  encoder.layer.5.intermediate.dense.weight \t torch.Size([4096, 1024])\n",
      "p:  encoder.layers.5.intermediate.dense.weight \t [1024, 4096] \n",
      "\n",
      "t:  encoder.layer.5.intermediate.dense.bias \t torch.Size([4096])\n",
      "p:  encoder.layers.5.intermediate.dense.bias \t [4096] \n",
      "\n",
      "transpose(permute) ---------->\n",
      "t:  encoder.layer.5.output.dense.weight \t torch.Size([1024, 4096])\n",
      "p:  encoder.layers.5.output.dense.weight \t [4096, 1024] \n",
      "\n",
      "t:  encoder.layer.5.output.dense.bias \t torch.Size([1024])\n",
      "p:  encoder.layers.5.output.dense.bias \t [1024] \n",
      "\n",
      "t:  encoder.layer.5.output.LayerNorm.weight \t torch.Size([1024])\n",
      "p:  encoder.layers.5.output.layer_norm.weight \t [1024] \n",
      "\n",
      "t:  encoder.layer.5.output.LayerNorm.bias \t torch.Size([1024])\n",
      "p:  encoder.layers.5.output.layer_norm.bias \t [1024] \n",
      "\n",
      "t:  encoder.layer.6.fourier.output.LayerNorm.weight \t torch.Size([1024])\n",
      "p:  encoder.layers.6.fourier.output.layer_norm.weight \t [1024] \n",
      "\n",
      "t:  encoder.layer.6.fourier.output.LayerNorm.bias \t torch.Size([1024])\n",
      "p:  encoder.layers.6.fourier.output.layer_norm.bias \t [1024] \n",
      "\n",
      "transpose(permute) ---------->\n",
      "t:  encoder.layer.6.intermediate.dense.weight \t torch.Size([4096, 1024])\n",
      "p:  encoder.layers.6.intermediate.dense.weight \t [1024, 4096] \n",
      "\n",
      "t:  encoder.layer.6.intermediate.dense.bias \t torch.Size([4096])\n",
      "p:  encoder.layers.6.intermediate.dense.bias \t [4096] \n",
      "\n",
      "transpose(permute) ---------->\n",
      "t:  encoder.layer.6.output.dense.weight \t torch.Size([1024, 4096])\n",
      "p:  encoder.layers.6.output.dense.weight \t [4096, 1024] \n",
      "\n",
      "t:  encoder.layer.6.output.dense.bias \t torch.Size([1024])\n",
      "p:  encoder.layers.6.output.dense.bias \t [1024] \n",
      "\n",
      "t:  encoder.layer.6.output.LayerNorm.weight \t torch.Size([1024])\n",
      "p:  encoder.layers.6.output.layer_norm.weight \t [1024] \n",
      "\n",
      "t:  encoder.layer.6.output.LayerNorm.bias \t torch.Size([1024])\n",
      "p:  encoder.layers.6.output.layer_norm.bias \t [1024] \n",
      "\n",
      "t:  encoder.layer.7.fourier.output.LayerNorm.weight \t torch.Size([1024])\n",
      "p:  encoder.layers.7.fourier.output.layer_norm.weight \t [1024] \n",
      "\n",
      "t:  encoder.layer.7.fourier.output.LayerNorm.bias \t torch.Size([1024])\n",
      "p:  encoder.layers.7.fourier.output.layer_norm.bias \t [1024] \n",
      "\n",
      "transpose(permute) ---------->\n",
      "t:  encoder.layer.7.intermediate.dense.weight \t torch.Size([4096, 1024])\n",
      "p:  encoder.layers.7.intermediate.dense.weight \t [1024, 4096] \n",
      "\n",
      "t:  encoder.layer.7.intermediate.dense.bias \t torch.Size([4096])\n",
      "p:  encoder.layers.7.intermediate.dense.bias \t [4096] \n",
      "\n",
      "transpose(permute) ---------->\n",
      "t:  encoder.layer.7.output.dense.weight \t torch.Size([1024, 4096])\n",
      "p:  encoder.layers.7.output.dense.weight \t [4096, 1024] \n",
      "\n",
      "t:  encoder.layer.7.output.dense.bias \t torch.Size([1024])\n",
      "p:  encoder.layers.7.output.dense.bias \t [1024] \n",
      "\n",
      "t:  encoder.layer.7.output.LayerNorm.weight \t torch.Size([1024])\n",
      "p:  encoder.layers.7.output.layer_norm.weight \t [1024] \n",
      "\n",
      "t:  encoder.layer.7.output.LayerNorm.bias \t torch.Size([1024])\n",
      "p:  encoder.layers.7.output.layer_norm.bias \t [1024] \n",
      "\n",
      "t:  encoder.layer.8.fourier.output.LayerNorm.weight \t torch.Size([1024])\n",
      "p:  encoder.layers.8.fourier.output.layer_norm.weight \t [1024] \n",
      "\n",
      "t:  encoder.layer.8.fourier.output.LayerNorm.bias \t torch.Size([1024])\n",
      "p:  encoder.layers.8.fourier.output.layer_norm.bias \t [1024] \n",
      "\n",
      "transpose(permute) ---------->\n",
      "t:  encoder.layer.8.intermediate.dense.weight \t torch.Size([4096, 1024])\n",
      "p:  encoder.layers.8.intermediate.dense.weight \t [1024, 4096] \n",
      "\n",
      "t:  encoder.layer.8.intermediate.dense.bias \t torch.Size([4096])\n",
      "p:  encoder.layers.8.intermediate.dense.bias \t [4096] \n",
      "\n",
      "transpose(permute) ---------->\n",
      "t:  encoder.layer.8.output.dense.weight \t torch.Size([1024, 4096])\n",
      "p:  encoder.layers.8.output.dense.weight \t [4096, 1024] \n",
      "\n",
      "t:  encoder.layer.8.output.dense.bias \t torch.Size([1024])\n",
      "p:  encoder.layers.8.output.dense.bias \t [1024] \n",
      "\n",
      "t:  encoder.layer.8.output.LayerNorm.weight \t torch.Size([1024])\n",
      "p:  encoder.layers.8.output.layer_norm.weight \t [1024] \n",
      "\n",
      "t:  encoder.layer.8.output.LayerNorm.bias \t torch.Size([1024])\n",
      "p:  encoder.layers.8.output.layer_norm.bias \t [1024] \n",
      "\n",
      "t:  encoder.layer.9.fourier.output.LayerNorm.weight \t torch.Size([1024])\n",
      "p:  encoder.layers.9.fourier.output.layer_norm.weight \t [1024] \n",
      "\n",
      "t:  encoder.layer.9.fourier.output.LayerNorm.bias \t torch.Size([1024])\n",
      "p:  encoder.layers.9.fourier.output.layer_norm.bias \t [1024] \n",
      "\n",
      "transpose(permute) ---------->\n",
      "t:  encoder.layer.9.intermediate.dense.weight \t torch.Size([4096, 1024])\n",
      "p:  encoder.layers.9.intermediate.dense.weight \t [1024, 4096] \n",
      "\n",
      "t:  encoder.layer.9.intermediate.dense.bias \t torch.Size([4096])\n",
      "p:  encoder.layers.9.intermediate.dense.bias \t [4096] \n",
      "\n",
      "transpose(permute) ---------->\n",
      "t:  encoder.layer.9.output.dense.weight \t torch.Size([1024, 4096])\n",
      "p:  encoder.layers.9.output.dense.weight \t [4096, 1024] \n",
      "\n",
      "t:  encoder.layer.9.output.dense.bias \t torch.Size([1024])\n",
      "p:  encoder.layers.9.output.dense.bias \t [1024] \n",
      "\n",
      "t:  encoder.layer.9.output.LayerNorm.weight \t torch.Size([1024])\n",
      "p:  encoder.layers.9.output.layer_norm.weight \t [1024] \n",
      "\n",
      "t:  encoder.layer.9.output.LayerNorm.bias \t torch.Size([1024])\n",
      "p:  encoder.layers.9.output.layer_norm.bias \t [1024] \n",
      "\n",
      "t:  encoder.layer.10.fourier.output.LayerNorm.weight \t torch.Size([1024])\n",
      "p:  encoder.layers.10.fourier.output.layer_norm.weight \t [1024] \n",
      "\n",
      "t:  encoder.layer.10.fourier.output.LayerNorm.bias \t torch.Size([1024])\n",
      "p:  encoder.layers.10.fourier.output.layer_norm.bias \t [1024] \n",
      "\n",
      "transpose(permute) ---------->\n",
      "t:  encoder.layer.10.intermediate.dense.weight \t torch.Size([4096, 1024])\n",
      "p:  encoder.layers.10.intermediate.dense.weight \t [1024, 4096] \n",
      "\n",
      "t:  encoder.layer.10.intermediate.dense.bias \t torch.Size([4096])\n",
      "p:  encoder.layers.10.intermediate.dense.bias \t [4096] \n",
      "\n",
      "transpose(permute) ---------->\n",
      "t:  encoder.layer.10.output.dense.weight \t torch.Size([1024, 4096])\n",
      "p:  encoder.layers.10.output.dense.weight \t [4096, 1024] \n",
      "\n",
      "t:  encoder.layer.10.output.dense.bias \t torch.Size([1024])\n",
      "p:  encoder.layers.10.output.dense.bias \t [1024] \n",
      "\n",
      "t:  encoder.layer.10.output.LayerNorm.weight \t torch.Size([1024])\n",
      "p:  encoder.layers.10.output.layer_norm.weight \t [1024] \n",
      "\n",
      "t:  encoder.layer.10.output.LayerNorm.bias \t torch.Size([1024])\n",
      "p:  encoder.layers.10.output.layer_norm.bias \t [1024] \n",
      "\n",
      "t:  encoder.layer.11.fourier.output.LayerNorm.weight \t torch.Size([1024])\n",
      "p:  encoder.layers.11.fourier.output.layer_norm.weight \t [1024] \n",
      "\n",
      "t:  encoder.layer.11.fourier.output.LayerNorm.bias \t torch.Size([1024])\n",
      "p:  encoder.layers.11.fourier.output.layer_norm.bias \t [1024] \n",
      "\n",
      "transpose(permute) ---------->\n",
      "t:  encoder.layer.11.intermediate.dense.weight \t torch.Size([4096, 1024])\n",
      "p:  encoder.layers.11.intermediate.dense.weight \t [1024, 4096] \n",
      "\n",
      "t:  encoder.layer.11.intermediate.dense.bias \t torch.Size([4096])\n",
      "p:  encoder.layers.11.intermediate.dense.bias \t [4096] \n",
      "\n",
      "transpose(permute) ---------->\n",
      "t:  encoder.layer.11.output.dense.weight \t torch.Size([1024, 4096])\n",
      "p:  encoder.layers.11.output.dense.weight \t [4096, 1024] \n",
      "\n",
      "t:  encoder.layer.11.output.dense.bias \t torch.Size([1024])\n",
      "p:  encoder.layers.11.output.dense.bias \t [1024] \n",
      "\n",
      "t:  encoder.layer.11.output.LayerNorm.weight \t torch.Size([1024])\n",
      "p:  encoder.layers.11.output.layer_norm.weight \t [1024] \n",
      "\n",
      "t:  encoder.layer.11.output.LayerNorm.bias \t torch.Size([1024])\n",
      "p:  encoder.layers.11.output.layer_norm.bias \t [1024] \n",
      "\n",
      "t:  encoder.layer.12.fourier.output.LayerNorm.weight \t torch.Size([1024])\n",
      "p:  encoder.layers.12.fourier.output.layer_norm.weight \t [1024] \n",
      "\n",
      "t:  encoder.layer.12.fourier.output.LayerNorm.bias \t torch.Size([1024])\n",
      "p:  encoder.layers.12.fourier.output.layer_norm.bias \t [1024] \n",
      "\n",
      "transpose(permute) ---------->\n",
      "t:  encoder.layer.12.intermediate.dense.weight \t torch.Size([4096, 1024])\n",
      "p:  encoder.layers.12.intermediate.dense.weight \t [1024, 4096] \n",
      "\n",
      "t:  encoder.layer.12.intermediate.dense.bias \t torch.Size([4096])\n",
      "p:  encoder.layers.12.intermediate.dense.bias \t [4096] \n",
      "\n",
      "transpose(permute) ---------->\n",
      "t:  encoder.layer.12.output.dense.weight \t torch.Size([1024, 4096])\n",
      "p:  encoder.layers.12.output.dense.weight \t [4096, 1024] \n",
      "\n",
      "t:  encoder.layer.12.output.dense.bias \t torch.Size([1024])\n",
      "p:  encoder.layers.12.output.dense.bias \t [1024] \n",
      "\n",
      "t:  encoder.layer.12.output.LayerNorm.weight \t torch.Size([1024])\n",
      "p:  encoder.layers.12.output.layer_norm.weight \t [1024] \n",
      "\n",
      "t:  encoder.layer.12.output.LayerNorm.bias \t torch.Size([1024])\n",
      "p:  encoder.layers.12.output.layer_norm.bias \t [1024] \n",
      "\n",
      "t:  encoder.layer.13.fourier.output.LayerNorm.weight \t torch.Size([1024])\n",
      "p:  encoder.layers.13.fourier.output.layer_norm.weight \t [1024] \n",
      "\n",
      "t:  encoder.layer.13.fourier.output.LayerNorm.bias \t torch.Size([1024])\n",
      "p:  encoder.layers.13.fourier.output.layer_norm.bias \t [1024] \n",
      "\n",
      "transpose(permute) ---------->\n",
      "t:  encoder.layer.13.intermediate.dense.weight \t torch.Size([4096, 1024])\n",
      "p:  encoder.layers.13.intermediate.dense.weight \t [1024, 4096] \n",
      "\n",
      "t:  encoder.layer.13.intermediate.dense.bias \t torch.Size([4096])\n",
      "p:  encoder.layers.13.intermediate.dense.bias \t [4096] \n",
      "\n",
      "transpose(permute) ---------->\n",
      "t:  encoder.layer.13.output.dense.weight \t torch.Size([1024, 4096])\n",
      "p:  encoder.layers.13.output.dense.weight \t [4096, 1024] \n",
      "\n",
      "t:  encoder.layer.13.output.dense.bias \t torch.Size([1024])\n",
      "p:  encoder.layers.13.output.dense.bias \t [1024] \n",
      "\n",
      "t:  encoder.layer.13.output.LayerNorm.weight \t torch.Size([1024])\n",
      "p:  encoder.layers.13.output.layer_norm.weight \t [1024] \n",
      "\n",
      "t:  encoder.layer.13.output.LayerNorm.bias \t torch.Size([1024])\n",
      "p:  encoder.layers.13.output.layer_norm.bias \t [1024] \n",
      "\n",
      "t:  encoder.layer.14.fourier.output.LayerNorm.weight \t torch.Size([1024])\n",
      "p:  encoder.layers.14.fourier.output.layer_norm.weight \t [1024] \n",
      "\n",
      "t:  encoder.layer.14.fourier.output.LayerNorm.bias \t torch.Size([1024])\n",
      "p:  encoder.layers.14.fourier.output.layer_norm.bias \t [1024] \n",
      "\n",
      "transpose(permute) ---------->\n",
      "t:  encoder.layer.14.intermediate.dense.weight \t torch.Size([4096, 1024])\n",
      "p:  encoder.layers.14.intermediate.dense.weight \t [1024, 4096] \n",
      "\n",
      "t:  encoder.layer.14.intermediate.dense.bias \t torch.Size([4096])\n",
      "p:  encoder.layers.14.intermediate.dense.bias \t [4096] \n",
      "\n",
      "transpose(permute) ---------->\n",
      "t:  encoder.layer.14.output.dense.weight \t torch.Size([1024, 4096])\n",
      "p:  encoder.layers.14.output.dense.weight \t [4096, 1024] \n",
      "\n",
      "t:  encoder.layer.14.output.dense.bias \t torch.Size([1024])\n",
      "p:  encoder.layers.14.output.dense.bias \t [1024] \n",
      "\n",
      "t:  encoder.layer.14.output.LayerNorm.weight \t torch.Size([1024])\n",
      "p:  encoder.layers.14.output.layer_norm.weight \t [1024] \n",
      "\n",
      "t:  encoder.layer.14.output.LayerNorm.bias \t torch.Size([1024])\n",
      "p:  encoder.layers.14.output.layer_norm.bias \t [1024] \n",
      "\n",
      "t:  encoder.layer.15.fourier.output.LayerNorm.weight \t torch.Size([1024])\n",
      "p:  encoder.layers.15.fourier.output.layer_norm.weight \t [1024] \n",
      "\n",
      "t:  encoder.layer.15.fourier.output.LayerNorm.bias \t torch.Size([1024])\n",
      "p:  encoder.layers.15.fourier.output.layer_norm.bias \t [1024] \n",
      "\n",
      "transpose(permute) ---------->\n",
      "t:  encoder.layer.15.intermediate.dense.weight \t torch.Size([4096, 1024])\n",
      "p:  encoder.layers.15.intermediate.dense.weight \t [1024, 4096] \n",
      "\n",
      "t:  encoder.layer.15.intermediate.dense.bias \t torch.Size([4096])\n",
      "p:  encoder.layers.15.intermediate.dense.bias \t [4096] \n",
      "\n",
      "transpose(permute) ---------->\n",
      "t:  encoder.layer.15.output.dense.weight \t torch.Size([1024, 4096])\n",
      "p:  encoder.layers.15.output.dense.weight \t [4096, 1024] \n",
      "\n",
      "t:  encoder.layer.15.output.dense.bias \t torch.Size([1024])\n",
      "p:  encoder.layers.15.output.dense.bias \t [1024] \n",
      "\n",
      "t:  encoder.layer.15.output.LayerNorm.weight \t torch.Size([1024])\n",
      "p:  encoder.layers.15.output.layer_norm.weight \t [1024] \n",
      "\n",
      "t:  encoder.layer.15.output.LayerNorm.bias \t torch.Size([1024])\n",
      "p:  encoder.layers.15.output.layer_norm.bias \t [1024] \n",
      "\n",
      "t:  encoder.layer.16.fourier.output.LayerNorm.weight \t torch.Size([1024])\n",
      "p:  encoder.layers.16.fourier.output.layer_norm.weight \t [1024] \n",
      "\n",
      "t:  encoder.layer.16.fourier.output.LayerNorm.bias \t torch.Size([1024])\n",
      "p:  encoder.layers.16.fourier.output.layer_norm.bias \t [1024] \n",
      "\n",
      "transpose(permute) ---------->\n",
      "t:  encoder.layer.16.intermediate.dense.weight \t torch.Size([4096, 1024])\n",
      "p:  encoder.layers.16.intermediate.dense.weight \t [1024, 4096] \n",
      "\n",
      "t:  encoder.layer.16.intermediate.dense.bias \t torch.Size([4096])\n",
      "p:  encoder.layers.16.intermediate.dense.bias \t [4096] \n",
      "\n",
      "transpose(permute) ---------->\n",
      "t:  encoder.layer.16.output.dense.weight \t torch.Size([1024, 4096])\n",
      "p:  encoder.layers.16.output.dense.weight \t [4096, 1024] \n",
      "\n",
      "t:  encoder.layer.16.output.dense.bias \t torch.Size([1024])\n",
      "p:  encoder.layers.16.output.dense.bias \t [1024] \n",
      "\n",
      "t:  encoder.layer.16.output.LayerNorm.weight \t torch.Size([1024])\n",
      "p:  encoder.layers.16.output.layer_norm.weight \t [1024] \n",
      "\n",
      "t:  encoder.layer.16.output.LayerNorm.bias \t torch.Size([1024])\n",
      "p:  encoder.layers.16.output.layer_norm.bias \t [1024] \n",
      "\n",
      "t:  encoder.layer.17.fourier.output.LayerNorm.weight \t torch.Size([1024])\n",
      "p:  encoder.layers.17.fourier.output.layer_norm.weight \t [1024] \n",
      "\n",
      "t:  encoder.layer.17.fourier.output.LayerNorm.bias \t torch.Size([1024])\n",
      "p:  encoder.layers.17.fourier.output.layer_norm.bias \t [1024] \n",
      "\n",
      "transpose(permute) ---------->\n",
      "t:  encoder.layer.17.intermediate.dense.weight \t torch.Size([4096, 1024])\n",
      "p:  encoder.layers.17.intermediate.dense.weight \t [1024, 4096] \n",
      "\n",
      "t:  encoder.layer.17.intermediate.dense.bias \t torch.Size([4096])\n",
      "p:  encoder.layers.17.intermediate.dense.bias \t [4096] \n",
      "\n",
      "transpose(permute) ---------->\n",
      "t:  encoder.layer.17.output.dense.weight \t torch.Size([1024, 4096])\n",
      "p:  encoder.layers.17.output.dense.weight \t [4096, 1024] \n",
      "\n",
      "t:  encoder.layer.17.output.dense.bias \t torch.Size([1024])\n",
      "p:  encoder.layers.17.output.dense.bias \t [1024] \n",
      "\n",
      "t:  encoder.layer.17.output.LayerNorm.weight \t torch.Size([1024])\n",
      "p:  encoder.layers.17.output.layer_norm.weight \t [1024] \n",
      "\n",
      "t:  encoder.layer.17.output.LayerNorm.bias \t torch.Size([1024])\n",
      "p:  encoder.layers.17.output.layer_norm.bias \t [1024] \n",
      "\n",
      "t:  encoder.layer.18.fourier.output.LayerNorm.weight \t torch.Size([1024])\n",
      "p:  encoder.layers.18.fourier.output.layer_norm.weight \t [1024] \n",
      "\n",
      "t:  encoder.layer.18.fourier.output.LayerNorm.bias \t torch.Size([1024])\n",
      "p:  encoder.layers.18.fourier.output.layer_norm.bias \t [1024] \n",
      "\n",
      "transpose(permute) ---------->\n",
      "t:  encoder.layer.18.intermediate.dense.weight \t torch.Size([4096, 1024])\n",
      "p:  encoder.layers.18.intermediate.dense.weight \t [1024, 4096] \n",
      "\n",
      "t:  encoder.layer.18.intermediate.dense.bias \t torch.Size([4096])\n",
      "p:  encoder.layers.18.intermediate.dense.bias \t [4096] \n",
      "\n",
      "transpose(permute) ---------->\n",
      "t:  encoder.layer.18.output.dense.weight \t torch.Size([1024, 4096])\n",
      "p:  encoder.layers.18.output.dense.weight \t [4096, 1024] \n",
      "\n",
      "t:  encoder.layer.18.output.dense.bias \t torch.Size([1024])\n",
      "p:  encoder.layers.18.output.dense.bias \t [1024] \n",
      "\n",
      "t:  encoder.layer.18.output.LayerNorm.weight \t torch.Size([1024])\n",
      "p:  encoder.layers.18.output.layer_norm.weight \t [1024] \n",
      "\n",
      "t:  encoder.layer.18.output.LayerNorm.bias \t torch.Size([1024])\n",
      "p:  encoder.layers.18.output.layer_norm.bias \t [1024] \n",
      "\n",
      "t:  encoder.layer.19.fourier.output.LayerNorm.weight \t torch.Size([1024])\n",
      "p:  encoder.layers.19.fourier.output.layer_norm.weight \t [1024] \n",
      "\n",
      "t:  encoder.layer.19.fourier.output.LayerNorm.bias \t torch.Size([1024])\n",
      "p:  encoder.layers.19.fourier.output.layer_norm.bias \t [1024] \n",
      "\n",
      "transpose(permute) ---------->\n",
      "t:  encoder.layer.19.intermediate.dense.weight \t torch.Size([4096, 1024])\n",
      "p:  encoder.layers.19.intermediate.dense.weight \t [1024, 4096] \n",
      "\n",
      "t:  encoder.layer.19.intermediate.dense.bias \t torch.Size([4096])\n",
      "p:  encoder.layers.19.intermediate.dense.bias \t [4096] \n",
      "\n",
      "transpose(permute) ---------->\n",
      "t:  encoder.layer.19.output.dense.weight \t torch.Size([1024, 4096])\n",
      "p:  encoder.layers.19.output.dense.weight \t [4096, 1024] \n",
      "\n",
      "t:  encoder.layer.19.output.dense.bias \t torch.Size([1024])\n",
      "p:  encoder.layers.19.output.dense.bias \t [1024] \n",
      "\n",
      "t:  encoder.layer.19.output.LayerNorm.weight \t torch.Size([1024])\n",
      "p:  encoder.layers.19.output.layer_norm.weight \t [1024] \n",
      "\n",
      "t:  encoder.layer.19.output.LayerNorm.bias \t torch.Size([1024])\n",
      "p:  encoder.layers.19.output.layer_norm.bias \t [1024] \n",
      "\n",
      "t:  encoder.layer.20.fourier.output.LayerNorm.weight \t torch.Size([1024])\n",
      "p:  encoder.layers.20.fourier.output.layer_norm.weight \t [1024] \n",
      "\n",
      "t:  encoder.layer.20.fourier.output.LayerNorm.bias \t torch.Size([1024])\n",
      "p:  encoder.layers.20.fourier.output.layer_norm.bias \t [1024] \n",
      "\n",
      "transpose(permute) ---------->\n",
      "t:  encoder.layer.20.intermediate.dense.weight \t torch.Size([4096, 1024])\n",
      "p:  encoder.layers.20.intermediate.dense.weight \t [1024, 4096] \n",
      "\n",
      "t:  encoder.layer.20.intermediate.dense.bias \t torch.Size([4096])\n",
      "p:  encoder.layers.20.intermediate.dense.bias \t [4096] \n",
      "\n",
      "transpose(permute) ---------->\n",
      "t:  encoder.layer.20.output.dense.weight \t torch.Size([1024, 4096])\n",
      "p:  encoder.layers.20.output.dense.weight \t [4096, 1024] \n",
      "\n",
      "t:  encoder.layer.20.output.dense.bias \t torch.Size([1024])\n",
      "p:  encoder.layers.20.output.dense.bias \t [1024] \n",
      "\n",
      "t:  encoder.layer.20.output.LayerNorm.weight \t torch.Size([1024])\n",
      "p:  encoder.layers.20.output.layer_norm.weight \t [1024] \n",
      "\n",
      "t:  encoder.layer.20.output.LayerNorm.bias \t torch.Size([1024])\n",
      "p:  encoder.layers.20.output.layer_norm.bias \t [1024] \n",
      "\n",
      "t:  encoder.layer.21.fourier.output.LayerNorm.weight \t torch.Size([1024])\n",
      "p:  encoder.layers.21.fourier.output.layer_norm.weight \t [1024] \n",
      "\n",
      "t:  encoder.layer.21.fourier.output.LayerNorm.bias \t torch.Size([1024])\n",
      "p:  encoder.layers.21.fourier.output.layer_norm.bias \t [1024] \n",
      "\n",
      "transpose(permute) ---------->\n",
      "t:  encoder.layer.21.intermediate.dense.weight \t torch.Size([4096, 1024])\n",
      "p:  encoder.layers.21.intermediate.dense.weight \t [1024, 4096] \n",
      "\n",
      "t:  encoder.layer.21.intermediate.dense.bias \t torch.Size([4096])\n",
      "p:  encoder.layers.21.intermediate.dense.bias \t [4096] \n",
      "\n",
      "transpose(permute) ---------->\n",
      "t:  encoder.layer.21.output.dense.weight \t torch.Size([1024, 4096])\n",
      "p:  encoder.layers.21.output.dense.weight \t [4096, 1024] \n",
      "\n",
      "t:  encoder.layer.21.output.dense.bias \t torch.Size([1024])\n",
      "p:  encoder.layers.21.output.dense.bias \t [1024] \n",
      "\n",
      "t:  encoder.layer.21.output.LayerNorm.weight \t torch.Size([1024])\n",
      "p:  encoder.layers.21.output.layer_norm.weight \t [1024] \n",
      "\n",
      "t:  encoder.layer.21.output.LayerNorm.bias \t torch.Size([1024])\n",
      "p:  encoder.layers.21.output.layer_norm.bias \t [1024] \n",
      "\n",
      "t:  encoder.layer.22.fourier.output.LayerNorm.weight \t torch.Size([1024])\n",
      "p:  encoder.layers.22.fourier.output.layer_norm.weight \t [1024] \n",
      "\n",
      "t:  encoder.layer.22.fourier.output.LayerNorm.bias \t torch.Size([1024])\n",
      "p:  encoder.layers.22.fourier.output.layer_norm.bias \t [1024] \n",
      "\n",
      "transpose(permute) ---------->\n",
      "t:  encoder.layer.22.intermediate.dense.weight \t torch.Size([4096, 1024])\n",
      "p:  encoder.layers.22.intermediate.dense.weight \t [1024, 4096] \n",
      "\n",
      "t:  encoder.layer.22.intermediate.dense.bias \t torch.Size([4096])\n",
      "p:  encoder.layers.22.intermediate.dense.bias \t [4096] \n",
      "\n",
      "transpose(permute) ---------->\n",
      "t:  encoder.layer.22.output.dense.weight \t torch.Size([1024, 4096])\n",
      "p:  encoder.layers.22.output.dense.weight \t [4096, 1024] \n",
      "\n",
      "t:  encoder.layer.22.output.dense.bias \t torch.Size([1024])\n",
      "p:  encoder.layers.22.output.dense.bias \t [1024] \n",
      "\n",
      "t:  encoder.layer.22.output.LayerNorm.weight \t torch.Size([1024])\n",
      "p:  encoder.layers.22.output.layer_norm.weight \t [1024] \n",
      "\n",
      "t:  encoder.layer.22.output.LayerNorm.bias \t torch.Size([1024])\n",
      "p:  encoder.layers.22.output.layer_norm.bias \t [1024] \n",
      "\n",
      "t:  encoder.layer.23.fourier.output.LayerNorm.weight \t torch.Size([1024])\n",
      "p:  encoder.layers.23.fourier.output.layer_norm.weight \t [1024] \n",
      "\n",
      "t:  encoder.layer.23.fourier.output.LayerNorm.bias \t torch.Size([1024])\n",
      "p:  encoder.layers.23.fourier.output.layer_norm.bias \t [1024] \n",
      "\n",
      "transpose(permute) ---------->\n",
      "t:  encoder.layer.23.intermediate.dense.weight \t torch.Size([4096, 1024])\n",
      "p:  encoder.layers.23.intermediate.dense.weight \t [1024, 4096] \n",
      "\n",
      "t:  encoder.layer.23.intermediate.dense.bias \t torch.Size([4096])\n",
      "p:  encoder.layers.23.intermediate.dense.bias \t [4096] \n",
      "\n",
      "transpose(permute) ---------->\n",
      "t:  encoder.layer.23.output.dense.weight \t torch.Size([1024, 4096])\n",
      "p:  encoder.layers.23.output.dense.weight \t [4096, 1024] \n",
      "\n",
      "t:  encoder.layer.23.output.dense.bias \t torch.Size([1024])\n",
      "p:  encoder.layers.23.output.dense.bias \t [1024] \n",
      "\n",
      "t:  encoder.layer.23.output.LayerNorm.weight \t torch.Size([1024])\n",
      "p:  encoder.layers.23.output.layer_norm.weight \t [1024] \n",
      "\n",
      "t:  encoder.layer.23.output.LayerNorm.bias \t torch.Size([1024])\n",
      "p:  encoder.layers.23.output.layer_norm.bias \t [1024] \n",
      "\n",
      "transpose(permute) ---------->\n",
      "t:  pooler.dense.weight \t torch.Size([1024, 1024])\n",
      "p:  pooler.dense.weight \t [1024, 1024] \n",
      "\n",
      "t:  pooler.dense.bias \t torch.Size([1024])\n",
      "p:  pooler.dense.bias \t [1024] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "import paddle\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "torch_base_model_path = 'pretrained_model/torch/base/pytorch_model.bin'\n",
    "torch_large_model_path = 'pretrained_model/torch/large/pytorch_model.bin'\n",
    "\n",
    "paddle_base_model_path = \"pretrained_model/temp/paddle/base/model_state.pdparams\"\n",
    "paddle_large_model_path = \"pretrained_model/temp/paddle/large/model_state.pdparams\"\n",
    "\n",
    "is_base = False\n",
    "\n",
    "if is_base:\n",
    "    torch_model_path = torch_base_model_path\n",
    "    paddle_model_path = paddle_base_model_path\n",
    "else:\n",
    "    torch_model_path = torch_large_model_path\n",
    "    paddle_model_path = paddle_large_model_path\n",
    "    \n",
    "torch_state_dict = torch.load(torch_model_path)\n",
    "\n",
    "paddle_state_dict = {}\n",
    "\n",
    "# State_dict's keys mapping: from torch to paddle\n",
    "keys_dict = {\n",
    "    # about encoder layer\n",
    "    'LayerNorm': 'layer_norm',\n",
    "    'encoder.layer': 'encoder.layers'\n",
    "}\n",
    "\n",
    "\n",
    "for torch_key in torch_state_dict:\n",
    "    paddle_key = torch_key\n",
    "    for k in keys_dict:\n",
    "        if k in paddle_key:\n",
    "            paddle_key = paddle_key.replace(k, keys_dict[k])\n",
    "\n",
    "    if ('map_fc' in paddle_key) or ('glyph_map' in paddle_key) or ('linear' in paddle_key) or ('proj' in  paddle_key) or ('vocab' in  paddle_key and 'weight' in  paddle_key) or (\"dense.weight\" in paddle_key) or ('transform.weight' in paddle_key) or ('seq_relationship.weight' in paddle_key):\n",
    "        print(\"transpose(permute) ---------->\")\n",
    "        paddle_state_dict[paddle_key] = paddle.to_tensor(torch_state_dict[torch_key].cpu().numpy().transpose())\n",
    "    else:\n",
    "        paddle_state_dict[paddle_key] = paddle.to_tensor(torch_state_dict[torch_key].cpu().numpy())\n",
    "\n",
    "    print(\"t: \", torch_key,\"\\t\", torch_state_dict[torch_key].shape)\n",
    "    print(\"p: \", paddle_key, \"\\t\", paddle_state_dict[paddle_key].shape, \"\\n\")\n",
    "\n",
    "paddle.save(paddle_state_dict, paddle_model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e41e926-fd37-45c3-8ed0-90050410577d",
   "metadata": {},
   "source": [
    "## 对比前项精度"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63fcb058-d7ac-47e0-ba0a-5b21d2903e10",
   "metadata": {},
   "source": [
    "### Base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9170fa73-e721-40c9-b805-7da2806477ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at google/fnet-base were not used when initializing FNetModel: ['cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.bias', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing FNetModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing FNetModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "# torch\n",
    "from transformers import FNetTokenizer, FNetModel\n",
    "torch_tokenizer = FNetTokenizer.from_pretrained(\"google/fnet-base\")\n",
    "torch_model = FNetModel.from_pretrained(\"google/fnet-base\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a613c595-c760-4789-a983-363245d17f6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"Replace me by any text you'd like.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "98320f5e-0a8b-47d5-9215-f62a43da0304",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch_prediction_logits shape:(1, 12, 768)\n",
      "torch_prediction_logits:[[[ 4.4527473  -0.10137583 -0.21348645 ...  0.36847726 -0.23560826\n",
      "   -0.25296995]\n",
      "  [ 0.18779421 -0.39948907  0.23660113 ...  0.19996837  0.2783861\n",
      "    0.27940997]\n",
      "  [ 0.17300639  0.0606944  -0.47870204 ...  0.05200686 -1.295673\n",
      "    0.578657  ]\n",
      "  ...\n",
      "  [ 0.17106001  0.00485597 -0.06762558 ... -0.3548019  -0.82001925\n",
      "    0.01953557]\n",
      "  [ 0.06859297  0.23336133 -0.57087415 ... -0.3022466  -0.58877695\n",
      "   -0.11472143]\n",
      "  [ 0.24348916  0.1635376   0.36463982 ...  0.5811312  -0.60914195\n",
      "   -0.32033262]]]\n"
     ]
    }
   ],
   "source": [
    "torch_model.eval()\n",
    "torch_inputs = torch_tokenizer(text, return_tensors=\"pt\")\n",
    "torch_outputs = torch_model(**torch_inputs)\n",
    "\n",
    "torch_logits = torch_outputs[0]\n",
    "torch_array = torch_logits.cpu().detach().numpy()\n",
    "print(\"torch_prediction_logits shape:{}\".format(torch_array.shape))\n",
    "print(\"torch_prediction_logits:{}\".format(torch_array))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f74f12e7-dee0-4766-aa65-42e21a392bf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# paddle\n",
    "import paddle\n",
    "import paddlenlp as ppnlp\n",
    "\n",
    "model_path = '/root/.paddlenlp/models/fnet-base/'\n",
    "paddle_tokenizer = ppnlp.transformers.FNetTokenizer.from_pretrained(model_path)\n",
    "\n",
    "paddle_model = ppnlp.transformers.FNetModel.from_pretrained(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "23da363b-ab46-40e2-8cf6-3dcf4c76a54e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "paddle_prediction_logits shape:(1, 12, 768)\n",
      "paddle_prediction_logits:[[[ 4.452679   -0.10139666 -0.21349066 ...  0.36848179 -0.23563094\n",
      "   -0.25295028]\n",
      "  [ 0.18779372 -0.39948893  0.23660143 ...  0.19996753  0.27838627\n",
      "    0.27941138]\n",
      "  [ 0.1730059   0.06069482 -0.47870392 ...  0.05200668 -1.2956748\n",
      "    0.5786573 ]\n",
      "  ...\n",
      "  [ 0.17105973  0.00485592 -0.06762674 ... -0.3548022  -0.8200205\n",
      "    0.01953554]\n",
      "  [ 0.06859197  0.23335953 -0.57087404 ... -0.30224594 -0.58877856\n",
      "   -0.11472122]\n",
      "  [ 0.2434903   0.1635379   0.3646408  ...  0.5811306  -0.60914266\n",
      "   -0.3203326 ]]]\n"
     ]
    }
   ],
   "source": [
    "paddle_model.eval()\n",
    "paddle_inputs = paddle_tokenizer(text)\n",
    "paddle_inputs = {k:paddle.to_tensor([v]) for (k, v) in paddle_inputs.items()}\n",
    "paddle_outputs = paddle_model(**paddle_inputs)\n",
    "\n",
    "paddle_logits = paddle_outputs[0]\n",
    "paddle_array = paddle_logits.numpy()\n",
    "print(\"paddle_prediction_logits shape:{}\".format(paddle_array.shape))\n",
    "print(\"paddle_prediction_logits:{}\".format(paddle_array))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "272222eb-f03e-43f7-a7c0-ce0e23c08d84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6.818771e-05\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "assert torch_array.shape == paddle_array.shape, \"the output logits should have the same shape, but got : {} and {} instead\".format(torch_array.shape, paddle_array.shape)\n",
    "diff = torch_array - paddle_array\n",
    "print(np.amax(abs(diff)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "574ca949-61dd-431a-b874-0b0493b491f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save pretrained model\n",
    "base_model = 'pretrained_model/paddle/base/'\n",
    "paddle_model.save_pretrained(base_model)\n",
    "paddle_tokenizer.save_pretrained(base_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "552d3eae-af45-4c97-b8c3-4756b697654b",
   "metadata": {},
   "source": [
    "### large"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e29429c8-b44c-4665-bf13-f8d1034148a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at google/fnet-large were not used when initializing FNetModel: ['cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.bias', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing FNetModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing FNetModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "# torch\n",
    "from transformers import FNetTokenizer, FNetModel\n",
    "torch_tokenizer = FNetTokenizer.from_pretrained(\"google/fnet-large\")\n",
    "torch_model = FNetModel.from_pretrained(\"google/fnet-large\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "3440fe50-1975-44d5-ae33-417e456238c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"Replace me by any text you'd like.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "388c2c8c-74d3-4fd4-aecc-f3bb56613a61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch_prediction_logits shape:(1, 12, 1024)\n",
      "torch_prediction_logits:[[[-0.1167793  -0.44756714 -0.51739615 ... -0.19486761 -0.42286307\n",
      "   -0.68805295]\n",
      "  [-0.05607523  0.16956906  0.731847   ... -0.23074943 -0.02952154\n",
      "    0.42456526]\n",
      "  [-0.09154158  0.23390733  0.12532961 ... -0.39507478  0.2645798\n",
      "    0.27237514]\n",
      "  ...\n",
      "  [ 0.29037276 -0.2935935  -0.72729474 ... -0.10259533  0.09559726\n",
      "   -0.27696082]\n",
      "  [ 0.07809267  0.3788107   0.45179468 ...  0.31190634  0.15828757\n",
      "    0.17926018]\n",
      "  [ 0.5804361  -0.18722543  0.3022339  ...  0.6312446   0.5981479\n",
      "    0.42219177]]]\n"
     ]
    }
   ],
   "source": [
    "torch_model.eval()\n",
    "torch_inputs = torch_tokenizer(text, return_tensors=\"pt\")\n",
    "torch_outputs = torch_model(**torch_inputs)\n",
    "\n",
    "torch_logits = torch_outputs[0]\n",
    "torch_array = torch_logits.cpu().detach().numpy()\n",
    "print(\"torch_prediction_logits shape:{}\".format(torch_array.shape))\n",
    "print(\"torch_prediction_logits:{}\".format(torch_array))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b2f13d2b-8094-45ee-b328-932b951dd158",
   "metadata": {},
   "outputs": [],
   "source": [
    "# paddle\n",
    "import paddle\n",
    "import paddlenlp as ppnlp\n",
    "\n",
    "model_path = '/root/.paddlenlp/models/fnet-large/'\n",
    "paddle_tokenizer = ppnlp.transformers.FNetTokenizer.from_pretrained(model_path)\n",
    "\n",
    "paddle_model = ppnlp.transformers.FNetModel.from_pretrained(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "6871160a-7904-48b2-abb0-dbb64cb06165",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "paddle_prediction_logits shape:(1, 12, 1024)\n",
      "paddle_prediction_logits:[[[-0.11681072 -0.44754764 -0.51738214 ... -0.19486131 -0.42285317\n",
      "   -0.68803596]\n",
      "  [-0.05607542  0.16956775  0.7318458  ... -0.23074819 -0.02952065\n",
      "    0.42456695]\n",
      "  [-0.09154156  0.23390746  0.12533087 ... -0.3950725   0.26458147\n",
      "    0.27237627]\n",
      "  ...\n",
      "  [ 0.29037338 -0.2935925  -0.7272925  ... -0.10259687  0.09559766\n",
      "   -0.27695963]\n",
      "  [ 0.07809278  0.37881133  0.45179617 ...  0.31190765  0.1582875\n",
      "    0.17926173]\n",
      "  [ 0.58043474 -0.18722285  0.30223408 ...  0.6312434   0.59814847\n",
      "    0.4221893 ]]]\n"
     ]
    }
   ],
   "source": [
    "paddle_model.eval()\n",
    "paddle_inputs = paddle_tokenizer(text)\n",
    "paddle_inputs = {k:paddle.to_tensor([v]) for (k, v) in paddle_inputs.items()}\n",
    "paddle_outputs = paddle_model(**paddle_inputs)\n",
    "\n",
    "paddle_logits = paddle_outputs[0]\n",
    "paddle_array = paddle_logits.numpy()\n",
    "print(\"paddle_prediction_logits shape:{}\".format(paddle_array.shape))\n",
    "print(\"paddle_prediction_logits:{}\".format(paddle_array))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "1f94258c-eb21-426f-af06-60614d0a8fe7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.000116825104\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "assert torch_array.shape == paddle_array.shape, \"the output logits should have the same shape, but got : {} and {} instead\".format(torch_array.shape, paddle_array.shape)\n",
    "diff = torch_array - paddle_array\n",
    "print(np.amax(abs(diff)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "e89e3ec7-9b08-4e8a-ac5e-4369a794d6f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save pretrained model\n",
    "base_model = 'pretrained_model/paddle/large/'\n",
    "paddle_model.save_pretrained(base_model)\n",
    "paddle_tokenizer.save_pretrained(base_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53536cf8-5b8d-4ec5-8ed9-c24db187ba63",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
