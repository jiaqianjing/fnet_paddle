{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2cd6aa35-6b9b-4d9f-8b41-4db436a93bd5",
   "metadata": {},
   "source": [
    "## 模型参数 torch 转换 paddle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "89906706-e028-4de8-ae6e-c278a7768a1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t:  embeddings.position_ids \t torch.Size([1, 512])\n",
      "p:  embeddings.position_ids \t [1, 512] \n",
      "\n",
      "t:  embeddings.word_embeddings.weight \t torch.Size([32000, 768])\n",
      "p:  embeddings.word_embeddings.weight \t [32000, 768] \n",
      "\n",
      "t:  embeddings.position_embeddings.weight \t torch.Size([512, 768])\n",
      "p:  embeddings.position_embeddings.weight \t [512, 768] \n",
      "\n",
      "t:  embeddings.token_type_embeddings.weight \t torch.Size([4, 768])\n",
      "p:  embeddings.token_type_embeddings.weight \t [4, 768] \n",
      "\n",
      "t:  embeddings.LayerNorm.weight \t torch.Size([768])\n",
      "p:  embeddings.layer_norm.weight \t [768] \n",
      "\n",
      "t:  embeddings.LayerNorm.bias \t torch.Size([768])\n",
      "p:  embeddings.layer_norm.bias \t [768] \n",
      "\n",
      "transpose(permute) ---------->\n",
      "t:  embeddings.projection.weight \t torch.Size([768, 768])\n",
      "p:  embeddings.projection.weight \t [768, 768] \n",
      "\n",
      "transpose(permute) ---------->\n",
      "t:  embeddings.projection.bias \t torch.Size([768])\n",
      "p:  embeddings.projection.bias \t [768] \n",
      "\n",
      "t:  encoder.layer.0.fourier.output.LayerNorm.weight \t torch.Size([768])\n",
      "p:  encoder.layers.0.fourier.output.layer_norm.weight \t [768] \n",
      "\n",
      "t:  encoder.layer.0.fourier.output.LayerNorm.bias \t torch.Size([768])\n",
      "p:  encoder.layers.0.fourier.output.layer_norm.bias \t [768] \n",
      "\n",
      "transpose(permute) ---------->\n",
      "t:  encoder.layer.0.intermediate.dense.weight \t torch.Size([3072, 768])\n",
      "p:  encoder.layers.0.intermediate.dense.weight \t [768, 3072] \n",
      "\n",
      "t:  encoder.layer.0.intermediate.dense.bias \t torch.Size([3072])\n",
      "p:  encoder.layers.0.intermediate.dense.bias \t [3072] \n",
      "\n",
      "transpose(permute) ---------->\n",
      "t:  encoder.layer.0.output.dense.weight \t torch.Size([768, 3072])\n",
      "p:  encoder.layers.0.output.dense.weight \t [3072, 768] \n",
      "\n",
      "t:  encoder.layer.0.output.dense.bias \t torch.Size([768])\n",
      "p:  encoder.layers.0.output.dense.bias \t [768] \n",
      "\n",
      "t:  encoder.layer.0.output.LayerNorm.weight \t torch.Size([768])\n",
      "p:  encoder.layers.0.output.layer_norm.weight \t [768] \n",
      "\n",
      "t:  encoder.layer.0.output.LayerNorm.bias \t torch.Size([768])\n",
      "p:  encoder.layers.0.output.layer_norm.bias \t [768] \n",
      "\n",
      "t:  encoder.layer.1.fourier.output.LayerNorm.weight \t torch.Size([768])\n",
      "p:  encoder.layers.1.fourier.output.layer_norm.weight \t [768] \n",
      "\n",
      "t:  encoder.layer.1.fourier.output.LayerNorm.bias \t torch.Size([768])\n",
      "p:  encoder.layers.1.fourier.output.layer_norm.bias \t [768] \n",
      "\n",
      "transpose(permute) ---------->\n",
      "t:  encoder.layer.1.intermediate.dense.weight \t torch.Size([3072, 768])\n",
      "p:  encoder.layers.1.intermediate.dense.weight \t [768, 3072] \n",
      "\n",
      "t:  encoder.layer.1.intermediate.dense.bias \t torch.Size([3072])\n",
      "p:  encoder.layers.1.intermediate.dense.bias \t [3072] \n",
      "\n",
      "transpose(permute) ---------->\n",
      "t:  encoder.layer.1.output.dense.weight \t torch.Size([768, 3072])\n",
      "p:  encoder.layers.1.output.dense.weight \t [3072, 768] \n",
      "\n",
      "t:  encoder.layer.1.output.dense.bias \t torch.Size([768])\n",
      "p:  encoder.layers.1.output.dense.bias \t [768] \n",
      "\n",
      "t:  encoder.layer.1.output.LayerNorm.weight \t torch.Size([768])\n",
      "p:  encoder.layers.1.output.layer_norm.weight \t [768] \n",
      "\n",
      "t:  encoder.layer.1.output.LayerNorm.bias \t torch.Size([768])\n",
      "p:  encoder.layers.1.output.layer_norm.bias \t [768] \n",
      "\n",
      "t:  encoder.layer.2.fourier.output.LayerNorm.weight \t torch.Size([768])\n",
      "p:  encoder.layers.2.fourier.output.layer_norm.weight \t [768] \n",
      "\n",
      "t:  encoder.layer.2.fourier.output.LayerNorm.bias \t torch.Size([768])\n",
      "p:  encoder.layers.2.fourier.output.layer_norm.bias \t [768] \n",
      "\n",
      "transpose(permute) ---------->\n",
      "t:  encoder.layer.2.intermediate.dense.weight \t torch.Size([3072, 768])\n",
      "p:  encoder.layers.2.intermediate.dense.weight \t [768, 3072] \n",
      "\n",
      "t:  encoder.layer.2.intermediate.dense.bias \t torch.Size([3072])\n",
      "p:  encoder.layers.2.intermediate.dense.bias \t [3072] \n",
      "\n",
      "transpose(permute) ---------->\n",
      "t:  encoder.layer.2.output.dense.weight \t torch.Size([768, 3072])\n",
      "p:  encoder.layers.2.output.dense.weight \t [3072, 768] \n",
      "\n",
      "t:  encoder.layer.2.output.dense.bias \t torch.Size([768])\n",
      "p:  encoder.layers.2.output.dense.bias \t [768] \n",
      "\n",
      "t:  encoder.layer.2.output.LayerNorm.weight \t torch.Size([768])\n",
      "p:  encoder.layers.2.output.layer_norm.weight \t [768] \n",
      "\n",
      "t:  encoder.layer.2.output.LayerNorm.bias \t torch.Size([768])\n",
      "p:  encoder.layers.2.output.layer_norm.bias \t [768] \n",
      "\n",
      "t:  encoder.layer.3.fourier.output.LayerNorm.weight \t torch.Size([768])\n",
      "p:  encoder.layers.3.fourier.output.layer_norm.weight \t [768] \n",
      "\n",
      "t:  encoder.layer.3.fourier.output.LayerNorm.bias \t torch.Size([768])\n",
      "p:  encoder.layers.3.fourier.output.layer_norm.bias \t [768] \n",
      "\n",
      "transpose(permute) ---------->\n",
      "t:  encoder.layer.3.intermediate.dense.weight \t torch.Size([3072, 768])\n",
      "p:  encoder.layers.3.intermediate.dense.weight \t [768, 3072] \n",
      "\n",
      "t:  encoder.layer.3.intermediate.dense.bias \t torch.Size([3072])\n",
      "p:  encoder.layers.3.intermediate.dense.bias \t [3072] \n",
      "\n",
      "transpose(permute) ---------->\n",
      "t:  encoder.layer.3.output.dense.weight \t torch.Size([768, 3072])\n",
      "p:  encoder.layers.3.output.dense.weight \t [3072, 768] \n",
      "\n",
      "t:  encoder.layer.3.output.dense.bias \t torch.Size([768])\n",
      "p:  encoder.layers.3.output.dense.bias \t [768] \n",
      "\n",
      "t:  encoder.layer.3.output.LayerNorm.weight \t torch.Size([768])\n",
      "p:  encoder.layers.3.output.layer_norm.weight \t [768] \n",
      "\n",
      "t:  encoder.layer.3.output.LayerNorm.bias \t torch.Size([768])\n",
      "p:  encoder.layers.3.output.layer_norm.bias \t [768] \n",
      "\n",
      "t:  encoder.layer.4.fourier.output.LayerNorm.weight \t torch.Size([768])\n",
      "p:  encoder.layers.4.fourier.output.layer_norm.weight \t [768] \n",
      "\n",
      "t:  encoder.layer.4.fourier.output.LayerNorm.bias \t torch.Size([768])\n",
      "p:  encoder.layers.4.fourier.output.layer_norm.bias \t [768] \n",
      "\n",
      "transpose(permute) ---------->\n",
      "t:  encoder.layer.4.intermediate.dense.weight \t torch.Size([3072, 768])\n",
      "p:  encoder.layers.4.intermediate.dense.weight \t [768, 3072] \n",
      "\n",
      "t:  encoder.layer.4.intermediate.dense.bias \t torch.Size([3072])\n",
      "p:  encoder.layers.4.intermediate.dense.bias \t [3072] \n",
      "\n",
      "transpose(permute) ---------->\n",
      "t:  encoder.layer.4.output.dense.weight \t torch.Size([768, 3072])\n",
      "p:  encoder.layers.4.output.dense.weight \t [3072, 768] \n",
      "\n",
      "t:  encoder.layer.4.output.dense.bias \t torch.Size([768])\n",
      "p:  encoder.layers.4.output.dense.bias \t [768] \n",
      "\n",
      "t:  encoder.layer.4.output.LayerNorm.weight \t torch.Size([768])\n",
      "p:  encoder.layers.4.output.layer_norm.weight \t [768] \n",
      "\n",
      "t:  encoder.layer.4.output.LayerNorm.bias \t torch.Size([768])\n",
      "p:  encoder.layers.4.output.layer_norm.bias \t [768] \n",
      "\n",
      "t:  encoder.layer.5.fourier.output.LayerNorm.weight \t torch.Size([768])\n",
      "p:  encoder.layers.5.fourier.output.layer_norm.weight \t [768] \n",
      "\n",
      "t:  encoder.layer.5.fourier.output.LayerNorm.bias \t torch.Size([768])\n",
      "p:  encoder.layers.5.fourier.output.layer_norm.bias \t [768] \n",
      "\n",
      "transpose(permute) ---------->\n",
      "t:  encoder.layer.5.intermediate.dense.weight \t torch.Size([3072, 768])\n",
      "p:  encoder.layers.5.intermediate.dense.weight \t [768, 3072] \n",
      "\n",
      "t:  encoder.layer.5.intermediate.dense.bias \t torch.Size([3072])\n",
      "p:  encoder.layers.5.intermediate.dense.bias \t [3072] \n",
      "\n",
      "transpose(permute) ---------->\n",
      "t:  encoder.layer.5.output.dense.weight \t torch.Size([768, 3072])\n",
      "p:  encoder.layers.5.output.dense.weight \t [3072, 768] \n",
      "\n",
      "t:  encoder.layer.5.output.dense.bias \t torch.Size([768])\n",
      "p:  encoder.layers.5.output.dense.bias \t [768] \n",
      "\n",
      "t:  encoder.layer.5.output.LayerNorm.weight \t torch.Size([768])\n",
      "p:  encoder.layers.5.output.layer_norm.weight \t [768] \n",
      "\n",
      "t:  encoder.layer.5.output.LayerNorm.bias \t torch.Size([768])\n",
      "p:  encoder.layers.5.output.layer_norm.bias \t [768] \n",
      "\n",
      "t:  encoder.layer.6.fourier.output.LayerNorm.weight \t torch.Size([768])\n",
      "p:  encoder.layers.6.fourier.output.layer_norm.weight \t [768] \n",
      "\n",
      "t:  encoder.layer.6.fourier.output.LayerNorm.bias \t torch.Size([768])\n",
      "p:  encoder.layers.6.fourier.output.layer_norm.bias \t [768] \n",
      "\n",
      "transpose(permute) ---------->\n",
      "t:  encoder.layer.6.intermediate.dense.weight \t torch.Size([3072, 768])\n",
      "p:  encoder.layers.6.intermediate.dense.weight \t [768, 3072] \n",
      "\n",
      "t:  encoder.layer.6.intermediate.dense.bias \t torch.Size([3072])\n",
      "p:  encoder.layers.6.intermediate.dense.bias \t [3072] \n",
      "\n",
      "transpose(permute) ---------->\n",
      "t:  encoder.layer.6.output.dense.weight \t torch.Size([768, 3072])\n",
      "p:  encoder.layers.6.output.dense.weight \t [3072, 768] \n",
      "\n",
      "t:  encoder.layer.6.output.dense.bias \t torch.Size([768])\n",
      "p:  encoder.layers.6.output.dense.bias \t [768] \n",
      "\n",
      "t:  encoder.layer.6.output.LayerNorm.weight \t torch.Size([768])\n",
      "p:  encoder.layers.6.output.layer_norm.weight \t [768] \n",
      "\n",
      "t:  encoder.layer.6.output.LayerNorm.bias \t torch.Size([768])\n",
      "p:  encoder.layers.6.output.layer_norm.bias \t [768] \n",
      "\n",
      "t:  encoder.layer.7.fourier.output.LayerNorm.weight \t torch.Size([768])\n",
      "p:  encoder.layers.7.fourier.output.layer_norm.weight \t [768] \n",
      "\n",
      "t:  encoder.layer.7.fourier.output.LayerNorm.bias \t torch.Size([768])\n",
      "p:  encoder.layers.7.fourier.output.layer_norm.bias \t [768] \n",
      "\n",
      "transpose(permute) ---------->\n",
      "t:  encoder.layer.7.intermediate.dense.weight \t torch.Size([3072, 768])\n",
      "p:  encoder.layers.7.intermediate.dense.weight \t [768, 3072] \n",
      "\n",
      "t:  encoder.layer.7.intermediate.dense.bias \t torch.Size([3072])\n",
      "p:  encoder.layers.7.intermediate.dense.bias \t [3072] \n",
      "\n",
      "transpose(permute) ---------->\n",
      "t:  encoder.layer.7.output.dense.weight \t torch.Size([768, 3072])\n",
      "p:  encoder.layers.7.output.dense.weight \t [3072, 768] \n",
      "\n",
      "t:  encoder.layer.7.output.dense.bias \t torch.Size([768])\n",
      "p:  encoder.layers.7.output.dense.bias \t [768] \n",
      "\n",
      "t:  encoder.layer.7.output.LayerNorm.weight \t torch.Size([768])\n",
      "p:  encoder.layers.7.output.layer_norm.weight \t [768] \n",
      "\n",
      "t:  encoder.layer.7.output.LayerNorm.bias \t torch.Size([768])\n",
      "p:  encoder.layers.7.output.layer_norm.bias \t [768] \n",
      "\n",
      "t:  encoder.layer.8.fourier.output.LayerNorm.weight \t torch.Size([768])\n",
      "p:  encoder.layers.8.fourier.output.layer_norm.weight \t [768] \n",
      "\n",
      "t:  encoder.layer.8.fourier.output.LayerNorm.bias \t torch.Size([768])\n",
      "p:  encoder.layers.8.fourier.output.layer_norm.bias \t [768] \n",
      "\n",
      "transpose(permute) ---------->\n",
      "t:  encoder.layer.8.intermediate.dense.weight \t torch.Size([3072, 768])\n",
      "p:  encoder.layers.8.intermediate.dense.weight \t [768, 3072] \n",
      "\n",
      "t:  encoder.layer.8.intermediate.dense.bias \t torch.Size([3072])\n",
      "p:  encoder.layers.8.intermediate.dense.bias \t [3072] \n",
      "\n",
      "transpose(permute) ---------->\n",
      "t:  encoder.layer.8.output.dense.weight \t torch.Size([768, 3072])\n",
      "p:  encoder.layers.8.output.dense.weight \t [3072, 768] \n",
      "\n",
      "t:  encoder.layer.8.output.dense.bias \t torch.Size([768])\n",
      "p:  encoder.layers.8.output.dense.bias \t [768] \n",
      "\n",
      "t:  encoder.layer.8.output.LayerNorm.weight \t torch.Size([768])\n",
      "p:  encoder.layers.8.output.layer_norm.weight \t [768] \n",
      "\n",
      "t:  encoder.layer.8.output.LayerNorm.bias \t torch.Size([768])\n",
      "p:  encoder.layers.8.output.layer_norm.bias \t [768] \n",
      "\n",
      "t:  encoder.layer.9.fourier.output.LayerNorm.weight \t torch.Size([768])\n",
      "p:  encoder.layers.9.fourier.output.layer_norm.weight \t [768] \n",
      "\n",
      "t:  encoder.layer.9.fourier.output.LayerNorm.bias \t torch.Size([768])\n",
      "p:  encoder.layers.9.fourier.output.layer_norm.bias \t [768] \n",
      "\n",
      "transpose(permute) ---------->\n",
      "t:  encoder.layer.9.intermediate.dense.weight \t torch.Size([3072, 768])\n",
      "p:  encoder.layers.9.intermediate.dense.weight \t [768, 3072] \n",
      "\n",
      "t:  encoder.layer.9.intermediate.dense.bias \t torch.Size([3072])\n",
      "p:  encoder.layers.9.intermediate.dense.bias \t [3072] \n",
      "\n",
      "transpose(permute) ---------->\n",
      "t:  encoder.layer.9.output.dense.weight \t torch.Size([768, 3072])\n",
      "p:  encoder.layers.9.output.dense.weight \t [3072, 768] \n",
      "\n",
      "t:  encoder.layer.9.output.dense.bias \t torch.Size([768])\n",
      "p:  encoder.layers.9.output.dense.bias \t [768] \n",
      "\n",
      "t:  encoder.layer.9.output.LayerNorm.weight \t torch.Size([768])\n",
      "p:  encoder.layers.9.output.layer_norm.weight \t [768] \n",
      "\n",
      "t:  encoder.layer.9.output.LayerNorm.bias \t torch.Size([768])\n",
      "p:  encoder.layers.9.output.layer_norm.bias \t [768] \n",
      "\n",
      "t:  encoder.layer.10.fourier.output.LayerNorm.weight \t torch.Size([768])\n",
      "p:  encoder.layers.10.fourier.output.layer_norm.weight \t [768] \n",
      "\n",
      "t:  encoder.layer.10.fourier.output.LayerNorm.bias \t torch.Size([768])\n",
      "p:  encoder.layers.10.fourier.output.layer_norm.bias \t [768] \n",
      "\n",
      "transpose(permute) ---------->\n",
      "t:  encoder.layer.10.intermediate.dense.weight \t torch.Size([3072, 768])\n",
      "p:  encoder.layers.10.intermediate.dense.weight \t [768, 3072] \n",
      "\n",
      "t:  encoder.layer.10.intermediate.dense.bias \t torch.Size([3072])\n",
      "p:  encoder.layers.10.intermediate.dense.bias \t [3072] \n",
      "\n",
      "transpose(permute) ---------->\n",
      "t:  encoder.layer.10.output.dense.weight \t torch.Size([768, 3072])\n",
      "p:  encoder.layers.10.output.dense.weight \t [3072, 768] \n",
      "\n",
      "t:  encoder.layer.10.output.dense.bias \t torch.Size([768])\n",
      "p:  encoder.layers.10.output.dense.bias \t [768] \n",
      "\n",
      "t:  encoder.layer.10.output.LayerNorm.weight \t torch.Size([768])\n",
      "p:  encoder.layers.10.output.layer_norm.weight \t [768] \n",
      "\n",
      "t:  encoder.layer.10.output.LayerNorm.bias \t torch.Size([768])\n",
      "p:  encoder.layers.10.output.layer_norm.bias \t [768] \n",
      "\n",
      "t:  encoder.layer.11.fourier.output.LayerNorm.weight \t torch.Size([768])\n",
      "p:  encoder.layers.11.fourier.output.layer_norm.weight \t [768] \n",
      "\n",
      "t:  encoder.layer.11.fourier.output.LayerNorm.bias \t torch.Size([768])\n",
      "p:  encoder.layers.11.fourier.output.layer_norm.bias \t [768] \n",
      "\n",
      "transpose(permute) ---------->\n",
      "t:  encoder.layer.11.intermediate.dense.weight \t torch.Size([3072, 768])\n",
      "p:  encoder.layers.11.intermediate.dense.weight \t [768, 3072] \n",
      "\n",
      "t:  encoder.layer.11.intermediate.dense.bias \t torch.Size([3072])\n",
      "p:  encoder.layers.11.intermediate.dense.bias \t [3072] \n",
      "\n",
      "transpose(permute) ---------->\n",
      "t:  encoder.layer.11.output.dense.weight \t torch.Size([768, 3072])\n",
      "p:  encoder.layers.11.output.dense.weight \t [3072, 768] \n",
      "\n",
      "t:  encoder.layer.11.output.dense.bias \t torch.Size([768])\n",
      "p:  encoder.layers.11.output.dense.bias \t [768] \n",
      "\n",
      "t:  encoder.layer.11.output.LayerNorm.weight \t torch.Size([768])\n",
      "p:  encoder.layers.11.output.layer_norm.weight \t [768] \n",
      "\n",
      "t:  encoder.layer.11.output.LayerNorm.bias \t torch.Size([768])\n",
      "p:  encoder.layers.11.output.layer_norm.bias \t [768] \n",
      "\n",
      "transpose(permute) ---------->\n",
      "t:  pooler.dense.weight \t torch.Size([768, 768])\n",
      "p:  pooler.dense.weight \t [768, 768] \n",
      "\n",
      "t:  pooler.dense.bias \t torch.Size([768])\n",
      "p:  pooler.dense.bias \t [768] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "import paddle\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "torch_base_model_path = 'pretrained_model/torch/base/pytorch_model.bin'\n",
    "torch_large_model_path = 'pretrained_model/torch/large/pytorch_model.bin'\n",
    "\n",
    "paddle_base_model_path = \"pretrained_model/paddle/base/model_state.pdparams\"\n",
    "paddle_large_model_path = \"pretrained_model/paddle/large/model_state.pdparams\"\n",
    "\n",
    "is_base = True\n",
    "\n",
    "if is_base:\n",
    "    torch_model_path = torch_base_model_path\n",
    "    paddle_model_path = paddle_base_model_path\n",
    "else:\n",
    "    torch_model_path = torch_large_model_path\n",
    "    paddle_model_path = paddle_large_model_path\n",
    "    \n",
    "torch_state_dict = torch.load(torch_model_path)\n",
    "\n",
    "paddle_state_dict = {}\n",
    "\n",
    "# State_dict's keys mapping: from torch to paddle\n",
    "keys_dict = {\n",
    "    # about encoder layer\n",
    "    'LayerNorm': 'layer_norm',\n",
    "    'encoder.layer': 'encoder.layers'\n",
    "}\n",
    "\n",
    "\n",
    "for torch_key in torch_state_dict:\n",
    "    paddle_key = torch_key\n",
    "    for k in keys_dict:\n",
    "        if k in paddle_key:\n",
    "            paddle_key = paddle_key.replace(k, keys_dict[k])\n",
    "\n",
    "    if ('map_fc' in paddle_key) or ('glyph_map' in paddle_key) or ('linear' in paddle_key) or ('proj' in  paddle_key) or ('vocab' in  paddle_key and 'weight' in  paddle_key) or (\"dense.weight\" in paddle_key) or ('transform.weight' in paddle_key) or ('seq_relationship.weight' in paddle_key):\n",
    "        print(\"transpose(permute) ---------->\")\n",
    "        paddle_state_dict[paddle_key] = paddle.to_tensor(torch_state_dict[torch_key].cpu().numpy().transpose())\n",
    "    else:\n",
    "        paddle_state_dict[paddle_key] = paddle.to_tensor(torch_state_dict[torch_key].cpu().numpy())\n",
    "\n",
    "    print(\"t: \", torch_key,\"\\t\", torch_state_dict[torch_key].shape)\n",
    "    print(\"p: \", paddle_key, \"\\t\", paddle_state_dict[paddle_key].shape, \"\\n\")\n",
    "\n",
    "paddle.save(paddle_state_dict, paddle_model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e41e926-fd37-45c3-8ed0-90050410577d",
   "metadata": {},
   "source": [
    "## 对比前项精度"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63fcb058-d7ac-47e0-ba0a-5b21d2903e10",
   "metadata": {},
   "source": [
    "### Base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9170fa73-e721-40c9-b805-7da2806477ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at google/fnet-base were not used when initializing FNetModel: ['cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.predictions.decoder.bias', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing FNetModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing FNetModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "# torch\n",
    "from transformers import FNetTokenizer, FNetModel\n",
    "torch_tokenizer = FNetTokenizer.from_pretrained(\"google/fnet-base\")\n",
    "torch_model = FNetModel.from_pretrained(\"google/fnet-base\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a613c595-c760-4789-a983-363245d17f6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"Replace me by any text you'd like.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "98320f5e-0a8b-47d5-9215-f62a43da0304",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch_prediction_logits shape:(1, 12, 768)\n",
      "torch_prediction_logits:[[[ 4.4527473  -0.10137583 -0.21348645 ...  0.36847726 -0.23560826\n",
      "   -0.25296995]\n",
      "  [ 0.18779421 -0.39948907  0.23660113 ...  0.19996837  0.2783861\n",
      "    0.27940997]\n",
      "  [ 0.17300639  0.0606944  -0.47870204 ...  0.05200686 -1.295673\n",
      "    0.578657  ]\n",
      "  ...\n",
      "  [ 0.17106001  0.00485597 -0.06762558 ... -0.3548019  -0.82001925\n",
      "    0.01953557]\n",
      "  [ 0.06859297  0.23336133 -0.57087415 ... -0.3022466  -0.58877695\n",
      "   -0.11472143]\n",
      "  [ 0.24348916  0.1635376   0.36463982 ...  0.5811312  -0.60914195\n",
      "   -0.32033262]]]\n"
     ]
    }
   ],
   "source": [
    "torch_model.eval()\n",
    "torch_inputs = torch_tokenizer(text, return_tensors=\"pt\")\n",
    "torch_outputs = torch_model(**torch_inputs)\n",
    "\n",
    "torch_logits = torch_outputs[0]\n",
    "torch_array = torch_logits.cpu().detach().numpy()\n",
    "print(\"torch_prediction_logits shape:{}\".format(torch_array.shape))\n",
    "print(\"torch_prediction_logits:{}\".format(torch_array))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f74f12e7-dee0-4766-aa65-42e21a392bf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# paddle\n",
    "import os, sys\n",
    "import paddle\n",
    "sys.path.append('/workspace/fnet_paddle/PaddleNLP')\n",
    "import paddlenlp as ppnlp\n",
    "torch_large_model_path = 'pretrained_model/torch/large'\n",
    "paddle_tokenizer = ppnlp.transformers.FNetTokenizer.from_pretrained(torch_large_model_path)\n",
    "\n",
    "paddle_model = ppnlp.transformers.FNetModel()\n",
    "params_file_path = \"pretrained_model/paddle/base/model_state.pdparams\"\n",
    "param_dict = paddle.load(params_file_path)\n",
    "paddle_model.load_dict(param_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "23da363b-ab46-40e2-8cf6-3dcf4c76a54e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "paddle_prediction_logits shape:(1, 12, 768)\n",
      "paddle_prediction_logits:[[[ 4.4526024e+00 -1.0144000e-01 -2.1346147e-01 ...  3.6845985e-01\n",
      "   -2.3564060e-01 -2.5293329e-01]\n",
      "  [ 1.8808359e-01 -3.9938882e-01  2.3694393e-01 ...  2.0020518e-01\n",
      "    2.7856782e-01  2.7908224e-01]\n",
      "  [ 1.7336315e-01  6.0770631e-02 -4.7835657e-01 ...  5.1935472e-02\n",
      "   -1.2956736e+00  5.7832038e-01]\n",
      "  ...\n",
      "  [ 1.7099635e-01  4.0750708e-03 -6.7396842e-02 ... -3.5490695e-01\n",
      "   -8.1969118e-01  1.9487010e-02]\n",
      "  [ 6.7856006e-02  2.3314802e-01 -5.7079792e-01 ... -3.0198133e-01\n",
      "   -5.8851326e-01 -1.1473138e-01]\n",
      "  [ 2.4360198e-01  1.6350693e-01  3.6484715e-01 ...  5.8138633e-01\n",
      "   -6.0873306e-01 -3.2016295e-01]]]\n"
     ]
    }
   ],
   "source": [
    "paddle_model.eval()\n",
    "paddle_inputs = paddle_tokenizer(text)\n",
    "paddle_inputs = {k:paddle.to_tensor([v]) for (k, v) in paddle_inputs.items()}\n",
    "paddle_outputs = paddle_model(**paddle_inputs)\n",
    "\n",
    "paddle_logits = paddle_outputs[0]\n",
    "paddle_array = paddle_logits.numpy()\n",
    "print(\"paddle_prediction_logits shape:{}\".format(paddle_array.shape))\n",
    "print(\"paddle_prediction_logits:{}\".format(paddle_array))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "272222eb-f03e-43f7-a7c0-ce0e23c08d84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0012811422\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "assert torch_array.shape == paddle_array.shape, \"the output logits should have the same shape, but got : {} and {} instead\".format(torch_array.shape, paddle_array.shape)\n",
    "diff = torch_array - paddle_array\n",
    "print(np.amax(abs(diff)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93ac67d7-d5b6-4500-ad73-e322ce44f2c0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
